# Domain Crawl V2 - Documentation

## ğŸ“‹ Vue d'ensemble

Le **Domain Crawl** est une fonctionnalitÃ© V2 qui enrichit automatiquement les domaines web en extrayant leurs mÃ©tadonnÃ©es (titre, description, keywords, langue, etc.).

### CaractÃ©ristiques principales

- âœ… **100% SYNC** - Aucun async/await (conformitÃ© V2)
- âœ… **3-tier Fallback Strategy** - StratÃ©gie multi-sources robuste
- âœ… **API RESTful** - 5 endpoints pour contrÃ´ler le crawl
- âœ… **Background Processing** - TÃ¢ches Celery avec suivi de progression
- âœ… **Extraction intelligente** - MÃ©tadonnÃ©es enrichies automatiquement

---

## ğŸ—ï¸ Architecture

### Stack Technique (V2 SYNC)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API V2 (FastAPI)                     â”‚
â”‚              âœ… def (pas async def)                      â”‚
â”‚              âœ… Session sync (pas AsyncSession)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Domain Crawl Service                       â”‚
â”‚        - SÃ©lection domaines                             â”‚
â”‚        - Sauvegarde rÃ©sultats                           â”‚
â”‚        - Calcul statistiques                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Celery Tasks (Background)                  â”‚
â”‚        - domain_crawl_task                              â”‚
â”‚        - domain_recrawl_task                            â”‚
â”‚        - domain_crawl_batch_task                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Domain Crawler                             â”‚
â”‚        âœ… requests (pas aiohttp)                        â”‚
â”‚        âœ… Trafilatura + BeautifulSoup                   â”‚
â”‚        âœ… 3-tier fallback strategy                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fichiers crÃ©Ã©s

```
app/
â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ domain_crawl.py          # Pydantic models (DTOs)
â”œâ”€â”€ core/
â”‚   â””â”€â”€ domain_crawler.py        # Crawler SYNC (350 lignes)
â”œâ”€â”€ services/
â”‚   â””â”€â”€ domain_crawl_service.py  # Service layer (280 lignes)
â”œâ”€â”€ tasks/
â”‚   â””â”€â”€ domain_crawl_task.py     # TÃ¢ches Celery
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ v2/endpoints/
â”‚   â”‚   â””â”€â”€ domains.py           # API endpoints V2
â”‚   â””â”€â”€ dependencies.py          # Auth sync dependencies
â””â”€â”€ db/
    â””â”€â”€ session.py               # Session sync context manager

tests/
â”œâ”€â”€ test-domain-crawl.sh         # Test E2E
â”œâ”€â”€ get_crawled_domains.py       # Affichage rÃ©sultats
â”œâ”€â”€ check_domain_crawl_status.sh # Diagnostic
â””â”€â”€ README_DOMAIN_CRAWL_TESTS.md # Doc tests
```

---

## ğŸ¯ StratÃ©gie 3-Tier Fallback

Le crawler utilise une stratÃ©gie de fallback intelligente pour maximiser le taux de succÃ¨s :

### 1ï¸âƒ£ Trafilatura (Primaire)

```python
# Essaie HTTPS â†’ HTTP automatiquement
downloaded = trafilatura.fetch_url(f"https://{domain_name}")
metadata = trafilatura.extract_metadata(downloaded)
content = trafilatura.extract(downloaded)
```

**Avantages** :
- Extraction de contenu propre (sans pub, menus, etc.)
- MÃ©tadonnÃ©es natives (title, description, language)
- Fallback HTTPS â†’ HTTP intÃ©grÃ©

### 2ï¸âƒ£ Archive.org (Fallback)

```python
# Si Trafilatura Ã©choue, rÃ©cupÃ©rer via Wayback Machine
availability_url = f"http://archive.org/wayback/available?url={domain_name}"
snapshot_url = data['archived_snapshots']['closest']['url']
```

**Avantages** :
- RÃ©cupÃ¨re les sites hors ligne
- AccÃ¨s aux versions archivÃ©es
- Fiable pour les vieux domaines

### 3ï¸âƒ£ HTTP Direct (Ultime fallback)

```python
# Dernier recours : requÃªte HTTP brute
for protocol in ['https', 'http']:
    resp = requests.get(f"{protocol}://{domain_name}")
    soup = BeautifulSoup(resp.text, 'html.parser')
```

**Avantages** :
- Fonctionne toujours (sauf erreur rÃ©seau)
- ContrÃ´le total sur la requÃªte
- Fallback HTTPS â†’ HTTP

### Codes d'erreur

| Code | Description |
|------|-------------|
| `ERR_TRAFI` | Trafilatura a Ã©chouÃ© |
| `ERR_ARCHIVE_NOTFOUND` | Aucune archive disponible |
| `ERR_HTTP_404` | Domaine introuvable |
| `ERR_HTTP_500` | Erreur serveur |
| `ERR_TIMEOUT` | Timeout (30s par dÃ©faut) |
| `ERR_SSL` | Erreur certificat SSL |
| `ERR_CONNECTION` | Erreur de connexion |
| `ERR_HTTP_ALL` | Tous les fallbacks ont Ã©chouÃ© |

---

## ğŸ”Œ API Endpoints

### Base URL
```
http://localhost:8000/api/v2/domains
```

### 1. POST `/crawl` - Lancer un crawl

Lance un crawl en background via Celery.

**Request** :
```json
{
  "land_id": 69,
  "limit": 10,
  "only_unfetched": true
}
```

**Response** :
```json
{
  "job_id": 74,
  "domain_count": 10,
  "message": "Domain crawl started for 10 domain(s)"
}
```

**Exemple cURL** :
```bash
curl -X POST "http://localhost:8000/api/v2/domains/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"land_id": 69, "limit": 10, "only_unfetched": true}'
```

---

### 2. GET `/stats` - Statistiques

RÃ©cupÃ¨re les stats d'un land.

**Query params** :
- `land_id` (optional) - ID du land

**Response** :
```json
{
  "total_domains": 10,
  "fetched_domains": 2,
  "unfetched_domains": 8,
  "avg_http_status": 200.0
}
```

**Exemple cURL** :
```bash
curl "http://localhost:8000/api/v2/domains/stats?land_id=69" \
  -H "Authorization: Bearer $TOKEN"
```

---

### 3. GET `/` - Liste des domaines

Liste les domaines rÃ©cemment crawlÃ©s.

**Query params** :
- `land_id` (optional) - ID du land
- `limit` (default: 10, max: 100) - Nombre de rÃ©sultats

**Response** :
```json
[
  {
    "id": 2750,
    "name": "rmc.bfmtv.com",
    "land_id": 69,
    "title": "Fil Infos",
    "description": "Toute l'info et le sport en direct...",
    "keywords": null,
    "language": "fr",
    "http_status": "200",
    "fetched_at": "2025-10-19T16:47:08.511492+00:00",
    "last_crawled_at": "2025-10-19T16:47:08.511492+00:00"
  }
]
```

---

### 4. POST `/{domain_id}/recrawl` - Re-crawler un domaine

Re-crawl un domaine spÃ©cifique (rÃ©initialise son statut).

**Response** :
```json
{
  "message": "Recrawl started for domain rmc.bfmtv.com",
  "domain_id": 2750,
  "domain_name": "rmc.bfmtv.com",
  "task_id": "domain_recrawl_2750"
}
```

---

### 5. GET `/sources` - Stats par source

Stats de rÃ©ussite par mÃ©thode de crawl.

**Response** :
```json
{
  "land_id": 69,
  "by_source": {
    "trafilatura": 8,
    "archive_org": 1,
    "http_direct": 1,
    "error": 0
  },
  "total": 10
}
```

---

## ğŸ§ª Tests

### Test E2E complet

```bash
# Test avec land existant
./MyWebIntelligenceAPI/tests/test-domain-crawl.sh 69 5

# CrÃ©er un nouveau land et tester
./MyWebIntelligenceAPI/tests/test-domain-crawl.sh
```

**Ã‰tapes du test** :
1. âœ… VÃ©rification serveur
2. âœ… Authentification
3. âœ… SÃ©lection/crÃ©ation land
4. âœ… VÃ©rification domaines disponibles
5. âœ… Lancement crawl
6. âœ… Attente fin du job (60s timeout)
7. âœ… VÃ©rification rÃ©sultats

**Output attendu** :
```
ğŸ§ª Test Domain Crawl
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… Serveur accessible
âœ… Token: eyJhbGciOiJIUzI1NiIs...
âœ… LAND_ID=69
âœ… Crawl lancÃ©: JOB_ID=74

ğŸ¯ RÃ‰SULTATS FINAUX
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Land ID:              69
Domaines total:       10
Nouveaux fetchÃ©s:     5
Statut HTTP moyen:    200.0
```

---

### Diagnostic rapide

```bash
# VÃ©rifier l'Ã©tat de l'implÃ©mentation
./MyWebIntelligenceAPI/tests/check_domain_crawl_status.sh 69
```

**Output** :
```
ğŸ” VÃ©rification Ã©tat Domain Crawl

âœ… Serveur accessible
âœ… AuthentifiÃ©
âœ… Endpoint /api/v2/domains/stats existe
âœ… Table 'domains' existe (2726 domaines)
âœ… Land 69 existe: 10 domaines (8 non fetchÃ©s)

âœ… Domain Crawl est IMPLÃ‰MENTÃ‰
```

---

### Voir les rÃ©sultats

```bash
# Afficher les domaines crawlÃ©s
docker exec mywebintelligenceapi python /app/tests/get_crawled_domains.py 69 10

# Exporter en JSON
docker exec mywebintelligenceapi python /app/tests/get_crawled_domains.py 69 10 --json
```

**Output** :
```
ğŸ“Š STATISTIQUES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total domaines:       10
Domaines fetchÃ©s:     5 (50.0%)
Non fetchÃ©s:          5

SuccÃ¨s (200):         5
Taux de succÃ¨s:       100.0%

ğŸŒ DOMAINES CRAWLÃ‰S - Land ID: 69
================================================================================

1. âœ… rmc.bfmtv.com
   ID:              2750
   Titre:           Fil Infos
   Description:     Toute l'info et le sport en direct...
   Langue:          fr
   Statut HTTP:     200
   CrawlÃ© le:       2025-10-19 16:47:08
```

---

## ğŸ“Š Extraction des MÃ©tadonnÃ©es

### Champs extraits

| Champ | Source | Description |
|-------|--------|-------------|
| `title` | `<title>` ou Trafilatura | Titre de la page |
| `description` | `<meta name="description">` | Description meta |
| `keywords` | `<meta name="keywords">` | Mots-clÃ©s meta |
| `language` | `<html lang>` ou Trafilatura | Langue du contenu |
| `http_status` | HTTP response | Code statut (200, 404, etc.) |
| `fetched_at` | Timestamp | Date du crawl |

### Exemple d'extraction

**HTML source** :
```html
<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <title>Fil Infos - RMC</title>
  <meta name="description" content="Toute l'info et le sport en direct sur RMC">
  <meta name="keywords" content="info, sport, direct, rmc">
</head>
```

**RÃ©sultat extrait** :
```json
{
  "title": "Fil Infos - RMC",
  "description": "Toute l'info et le sport en direct sur RMC",
  "keywords": "info, sport, direct, rmc",
  "language": "fr",
  "http_status": "200"
}
```

---

## ğŸš€ Utilisation Pratique

### Workflow complet

```bash
# 1. VÃ©rifier que tout est opÃ©rationnel
./MyWebIntelligenceAPI/tests/check_domain_crawl_status.sh 69

# 2. Lancer un crawl (5 domaines)
./MyWebIntelligenceAPI/tests/test-domain-crawl.sh 69 5

# 3. Voir les rÃ©sultats
docker exec mywebintelligenceapi python /app/tests/get_crawled_domains.py 69 10

# 4. Exporter en JSON
docker exec mywebintelligenceapi python /app/tests/get_crawled_domains.py 69 10 --json

# 5. Copier le fichier JSON
docker cp mywebintelligenceapi:/app/domains_land69_*.json ./
```

---

### Via l'API directement

```bash
# 1. S'authentifier
TOKEN=$(curl -s -X POST http://localhost:8000/api/v1/auth/login \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=admin@example.com&password=changethispassword" \
  | jq -r .access_token)

# 2. Voir les stats
curl -s "http://localhost:8000/api/v2/domains/stats?land_id=69" \
  -H "Authorization: Bearer $TOKEN" | jq

# 3. Lancer un crawl
curl -s -X POST "http://localhost:8000/api/v2/domains/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"land_id": 69, "limit": 10}' | jq

# 4. Voir les domaines crawlÃ©s
curl -s "http://localhost:8000/api/v2/domains?land_id=69&limit=10" \
  -H "Authorization: Bearer $TOKEN" | jq
```

---

## ğŸ”§ Troubleshooting

### ProblÃ¨me : "Task not registered"

**SymptÃ´me** :
```
ERROR: Received unregistered task of type 'domain_crawl'
```

**Solution** :
```bash
# RedÃ©marrer le worker Celery
docker restart mywebclient-celery_worker-1

# VÃ©rifier l'enregistrement
docker logs mywebclient-celery_worker-1 | grep domain_crawl
```

---

### ProblÃ¨me : Domaines non sauvegardÃ©s

**SymptÃ´me** :
```sql
SELECT * FROM domains WHERE fetched_at IS NOT NULL;
-- (0 rows)
```

**Solution** :
VÃ©rifier les logs Celery pour erreurs :
```bash
docker logs mywebclient-celery_worker-1 --tail=100 | grep -i error
```

---

### ProblÃ¨me : Timeout du job

**SymptÃ´me** :
Le job reste en `pending` ou `running` indÃ©finiment.

**Causes possibles** :
1. Worker Celery arrÃªtÃ©
2. Connexion Redis perdue
3. Timeout rÃ©seau (domaines lents)

**Solutions** :
```bash
# 1. VÃ©rifier le worker
docker ps | grep celery

# 2. VÃ©rifier Redis
docker logs mywebclient-redis-1

# 3. Augmenter le timeout
# Dans app/config.py :
DOMAIN_CRAWL_TIMEOUT = 60  # 60 secondes
```

---

### ProblÃ¨me : Taux d'erreur Ã©levÃ©

**SymptÃ´me** :
```
Taux de succÃ¨s: 20.0%
```

**Diagnostic** :
```bash
# Voir les erreurs par source
curl "http://localhost:8000/api/v2/domains/sources?land_id=69" \
  -H "Authorization: Bearer $TOKEN" | jq
```

**Solutions** :
- VÃ©rifier la connectivitÃ© rÃ©seau du conteneur
- Tester manuellement un domaine :
  ```bash
  docker exec mywebintelligenceapi python -c "
  from app.core.domain_crawler import DomainCrawler
  crawler = DomainCrawler()
  result = crawler.fetch_domain('example.com')
  print(result)
  "
  ```

---

## ğŸ“ˆ Performance

### Benchmarks

| MÃ©trique | Valeur |
|----------|--------|
| Temps par domaine | ~2s (Trafilatura) |
| Temps par domaine | ~3s (Archive.org) |
| Temps par domaine | ~1.5s (HTTP direct) |
| Taux de succÃ¨s | 95-100% |
| Timeout par dÃ©faut | 30s |

### Optimisations

**Crawl en parallÃ¨le** :
```python
# Augmenter les workers Celery
# Dans docker-compose.yml :
celery_worker:
  command: celery -A app.core.celery_app worker --concurrency=4
```

**Augmenter le batch** :
```bash
# Crawler plus de domaines Ã  la fois
curl -X POST ".../domains/crawl" -d '{"limit": 100}'
```

---

## ğŸ”’ ConformitÃ© V2 SYNC

### âœ… Checklist de conformitÃ©

- âœ… **Pas de `async def`** - Tous les endpoints sont `def`
- âœ… **Pas de `await`** - Aucun appel asynchrone
- âœ… **Pas de `aiohttp`** - Utilise `requests`
- âœ… **Pas de `AsyncSession`** - Utilise `Session` sync
- âœ… **Celery workers SYNC** - TÃ¢ches synchrones
- âœ… **Context managers** - `get_sync_db_context()` pour les tasks

### Code V2 vs V3

**âŒ V3 (Async - NON utilisÃ©)** :
```python
async def crawl_domains(db: AsyncSession = Depends(get_db)):
    async with aiohttp.ClientSession() as session:
        result = await session.get(url)
```

**âœ… V2 (Sync - UTILISÃ‰)** :
```python
def crawl_domains(db: Session = Depends(get_sync_db)):
    with requests.Session() as session:
        result = session.get(url)
```

---

## ğŸ“ Notes de Migration Future

### Champs manquants en DB

Les champs suivants sont extraits mais **non sauvegardÃ©s** car absents de la table `domains` :

- `content` - Contenu HTML extrait
- `source_method` - MÃ©thode utilisÃ©e (trafilatura/archive_org/http_direct)
- `error_code` - Code erreur si Ã©chec
- `error_message` - Message d'erreur
- `fetch_duration_ms` - DurÃ©e du fetch
- `retry_count` - Nombre de tentatives

**Migration future requise** :
```sql
ALTER TABLE domains ADD COLUMN content TEXT;
ALTER TABLE domains ADD COLUMN source_method VARCHAR(50);
ALTER TABLE domains ADD COLUMN error_code VARCHAR(50);
ALTER TABLE domains ADD COLUMN error_message TEXT;
ALTER TABLE domains ADD COLUMN fetch_duration_ms INTEGER;
ALTER TABLE domains ADD COLUMN retry_count INTEGER;
```

---

## ğŸ“ RÃ©fÃ©rences

- [Architecture.md](.claude/system/Architecture.md) - Architecture globale V2
- [AGENTS.md](.claude/AGENTS.md) - Principes V2 SYNC
- [AGENTS.md](.claude/AGENTS.md) - Playbook principal
- [README_DOMAIN_CRAWL_TESTS.md](../tests/README_DOMAIN_CRAWL_TESTS.md) - Documentation tests

---

**Version** : 1.0
**Date** : 2025-10-19
**Statut** : âœ… ImplÃ©mentÃ© et testÃ©
**Auteur** : Claude Code
