# Tests Domain Crawl

Tests automatisÃ©s pour valider le systÃ¨me de crawl de domaines.

## ğŸ“ Fichiers

### 1. `test-domain-crawl.sh`
Script bash end-to-end qui teste le workflow complet du domain crawl.

**FonctionnalitÃ©s:**
- âœ… VÃ©rifie l'accessibilitÃ© du serveur
- âœ… Authentifie l'utilisateur
- âœ… CrÃ©e un land de test (ou utilise un existant)
- âœ… VÃ©rifie les domaines disponibles
- âœ… Lance le crawl des domaines
- âœ… Surveille la progression du job
- âœ… Affiche les statistiques avant/aprÃ¨s
- âœ… RÃ©cupÃ¨re les dÃ©tails du job

### 2. `get_crawled_domains.py`
Script Python pour rÃ©cupÃ©rer et afficher les domaines crawlÃ©s.

**FonctionnalitÃ©s:**
- âœ… Liste les domaines crawlÃ©s d'un land
- âœ… Affiche les mÃ©tadonnÃ©es (titre, description, keywords, http_status, etc.)
- âœ… Montre les statistiques globales
- âœ… Exporte en JSON (optionnel)

---

## ğŸš€ Utilisation

### Test complet du Domain Crawl

#### Option 1: CrÃ©er un nouveau land
```bash
cd MyWebClient
./MyWebIntelligenceAPI/tests/test-domain-crawl.sh
```

#### Option 2: Utiliser un land existant
```bash
./MyWebIntelligenceAPI/tests/test-domain-crawl.sh 69 10
#                                                 â”‚  â”‚
#                                                 â”‚  â””â”€ Limite (nombre de domaines)
#                                                 â””â”€â”€â”€â”€ Land ID
```

#### Variables d'environnement
```bash
# Changer l'URL de l'API
API_URL=http://production.com:8000 ./MyWebIntelligenceAPI/tests/test-domain-crawl.sh
```

---

### RÃ©cupÃ©rer les domaines crawlÃ©s

#### Usage de base
```bash
# Depuis le host
docker exec mywebintelligenceapi python tests/get_crawled_domains.py 69 10

# Depuis le container
python tests/get_crawled_domains.py 69 10
```

#### Options disponibles
```bash
# Avec export JSON
python tests/get_crawled_domains.py 69 10 --json

# Voir seulement les stats
python tests/get_crawled_domains.py 69 --stats

# Combiner les options
python tests/get_crawled_domains.py 69 20 --json --stats
```

---

## ğŸ“Š Exemple de sortie

### test-domain-crawl.sh

```
ğŸ§ª Test Domain Crawl - 2025-10-19 15:46:23
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”§ 1/7 - VÃ©rification serveur...
âœ… Serveur accessible

ğŸ”‘ 2/7 - Authentification...
âœ… Token: eyJhbGciOiJIUzI1NiIs...

ğŸ—ï¸ 3/7 - CrÃ©ation land de test...
âœ… Land crÃ©Ã©: LAND_ID=70

ğŸ“Š 4/7 - VÃ©rification domaines disponibles...
   Total domaines: 3
   Non fetchÃ©s: 3

ğŸ•·ï¸ 5/7 - Lancement Domain Crawl...
âœ… Crawl lancÃ©: JOB_ID=71
   Domaines Ã  crawler: 3
   Canal WebSocket: domain_crawl_progress_71

â³ 6/7 - Attente fin du crawl (max 60s)...
   Progression: 100% (completed)
âœ… Crawl terminÃ© avec succÃ¨s

ğŸ“Š 7/7 - VÃ©rification rÃ©sultats...

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ RÃ‰SULTATS FINAUX
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Land ID:              70
Job ID:               71
Statut Job:           completed

Domaines total:       3
Avant crawl:          3 non fetchÃ©s
AprÃ¨s crawl:          3 fetchÃ©s
Nouveaux fetchÃ©s:     3
Statut HTTP moyen:    200.0

DÃ©tails du crawl:
{
  "total": 3,
  "processed": 3,
  "success": 3,
  "errors": 0,
  "by_source": {
    "trafilatura": 2,
    "archive_org": 1,
    "http_direct": 0,
    "error": 0
  },
  "start_time": "2025-10-19T15:46:25.123456",
  "end_time": "2025-10-19T15:46:35.654321"
}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ Pour voir les domaines crawlÃ©s:
   docker exec mywebintelligenceapi python tests/get_crawled_domains.py 70 10

âœ… Test terminÃ©!
```

### get_crawled_domains.py

```
ğŸ“Š STATISTIQUES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total domaines:       3
Domaines fetchÃ©s:     3 (100.0%)
Non fetchÃ©s:          0

SuccÃ¨s (200):         2
Erreurs client (4xx): 1
Erreurs serveur (5xx):0
Taux de succÃ¨s:       66.67%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

================================================================================
ğŸŒ DOMAINES CRAWLÃ‰S - Land ID: 70
================================================================================

1. âœ… www.example.com
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ID:              123
   Titre:           Example Domain
   Description:     Example Domain. This domain is for use in illustrative...
   Mots-clÃ©s:       example, domain, test
   Langue:          en
   Statut HTTP:     200
   CrawlÃ© le:       2025-10-19 15:46:28
   Dernier crawl:   2025-10-19 15:46:28
   Source:          trafilatura
   DurÃ©e fetch:     1523 ms

2. âœ… www.wikipedia.org
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ID:              124
   Titre:           Wikipedia, the free encyclopedia
   Description:     Wikipedia is a free online encyclopedia, created and...
   Mots-clÃ©s:       wikipedia, encyclopedia, free
   Langue:          en
   Statut HTTP:     200
   CrawlÃ© le:       2025-10-19 15:46:32
   Dernier crawl:   2025-10-19 15:46:32
   Source:          trafilatura
   DurÃ©e fetch:     3421 ms

3. âŒ github.com
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ID:              125
   Titre:           GitHub: Let's build from here
   Description:     GitHub is where over 100 million developers shape...
   Mots-clÃ©s:       github, development, code
   Langue:          en
   Statut HTTP:     403
   CrawlÃ© le:       2025-10-19 15:46:34
   Dernier crawl:   2025-10-19 15:46:34
   Source:          archive_org
   DurÃ©e fetch:     2134 ms
   âš ï¸  Erreur:       ERR_HTTP_403 - Forbidden

================================================================================
Total affichÃ©: 3 domaine(s)
================================================================================

âœ… Successfully retrieved 3 domain(s).

ğŸ’¡ Options supplÃ©mentaires:
   --json, -j     Exporter en JSON
   --stats, -s    Afficher uniquement les stats

   Exemple: python tests/get_crawled_domains.py 70 10 --json
```

---

## ğŸ§ª ScÃ©narios de test

### 1. Test basic - Nouveau land
```bash
# CrÃ©e un land avec 3 domaines et les crawle
./tests/test-domain-crawl.sh
```

### 2. Test avec limite
```bash
# Crawl seulement 5 domaines d'un land existant
./tests/test-domain-crawl.sh 15 5
```

### 3. Test de performance
```bash
# Crawl 50 domaines et mesure le temps
time ./tests/test-domain-crawl.sh 15 50
```

### 4. VÃ©rification aprÃ¨s crawl
```bash
# 1. Lancer le crawl
./tests/test-domain-crawl.sh 69 10

# 2. VÃ©rifier les domaines crawlÃ©s
docker exec mywebintelligenceapi python tests/get_crawled_domains.py 69 10

# 3. Exporter en JSON
docker exec mywebintelligenceapi python tests/get_crawled_domains.py 69 10 --json

# 4. VÃ©rifier le fichier JSON
cat domains_land69_*.json | jq '.[] | {name, title, http_status}'
```

### 5. Debug - VÃ©rifier les logs
```bash
# Logs Celery pendant le crawl
docker logs mywebclient-celery_worker-1 --tail=100 -f

# Logs API
docker logs mywebintelligenceapi --tail=100 -f

# VÃ©rifier DB directement
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c \
  "SELECT name, title, http_status, fetched_at FROM domains WHERE land_id = 69 ORDER BY fetched_at DESC LIMIT 10;"
```

---

## âœ… Checklist de validation

Avant de considÃ©rer le domain crawl comme fonctionnel, vÃ©rifiez:

- [ ] Le script `test-domain-crawl.sh` se termine avec succÃ¨s
- [ ] Le job passe Ã  `completed` (pas `failed`)
- [ ] Au moins 80% des domaines ont `http_status = 200`
- [ ] Tous les domaines ont `fetched_at` dÃ©fini
- [ ] Les champs `title`, `description`, `language` sont remplis
- [ ] Les mÃ©tadonnÃ©es `source_method` sont correctes (trafilatura/archive_org/http_direct)
- [ ] Les erreurs ont des codes `ERR_*` appropriÃ©s
- [ ] Le script `get_crawled_domains.py` affiche correctement les donnÃ©es
- [ ] Les stats correspondent aux rÃ©sultats attendus
- [ ] Les logs Celery ne montrent pas d'erreurs critiques

---

## ğŸ› Troubleshooting

### ProblÃ¨me: "Aucun domaine Ã  crawler"
**Solution:** Le land n'a pas de domaines non-fetchÃ©s
```bash
# VÃ©rifier les domaines du land
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c \
  "SELECT COUNT(*) as total, COUNT(fetched_at) as fetched FROM domains WHERE land_id = 69;"

# RÃ©initialiser fetched_at si besoin (pour tests seulement!)
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c \
  "UPDATE domains SET fetched_at = NULL WHERE land_id = 69;"
```

### ProblÃ¨me: "Job timeout"
**Solution:** Augmentez le timeout ou vÃ©rifiez les workers Celery
```bash
# VÃ©rifier les workers
docker ps | grep celery

# Voir les logs des workers
docker logs mywebclient-celery_worker-1 --tail=50

# Relancer les workers si besoin
docker restart mywebclient-celery_worker-1
```

### ProblÃ¨me: "Authentification Ã©chouÃ©e"
**Solution:** VÃ©rifiez les credentials
```bash
# VÃ©rifier que l'utilisateur admin existe
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c \
  "SELECT email FROM users WHERE email = 'admin@example.com';"

# Si nÃ©cessaire, crÃ©er l'utilisateur admin via l'API ou Alembic
```

### ProblÃ¨me: "Tous les domaines Ã©chouent (http_status = 0)"
**Solution:** ProblÃ¨me rÃ©seau ou Trafilatura
```bash
# Test connexion depuis le container
docker exec mywebintelligenceapi curl -I https://www.example.com

# VÃ©rifier les dÃ©pendances
docker exec mywebintelligenceapi pip list | grep -E "trafilatura|aiohttp|beautifulsoup4"

# Voir les logs dÃ©taillÃ©s
docker logs mywebclient-celery_worker-1 | grep -i "domain"
```

---

## ğŸ“š RÃ©fÃ©rences

- [Plan de dÃ©veloppement Domain Crawl](../../.claude/tasks/domaincrawl_dev.md)
- [Architecture gÃ©nÃ©rale](.../../.claude/system/Architecture.md)
- [Guide des agents](.../../.claude/AGENTS.md)

---

**Auteur:** MyWebIntelligence Team
**DerniÃ¨re mise Ã  jour:** 2025-10-19
**Version:** 1.0
