#!/bin/bash
# Test ASYNC crawl - VÃ©rification alignement sync/async
# Teste les corrections: metadata, published_at, last_modified, etag, final_lang

set -e  # Exit on error

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo -e "${BLUE}   TEST CRAWLER ASYNC - VÃ©rification Alignement Sync/Async${NC}"
echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""

# Configuration
API_URL="${API_URL:-http://localhost:8000}"
DB_CONTAINER="${DB_CONTAINER:-mywebclient-db-1}"
DB_USER="${DB_USER:-mwi_user}"
DB_NAME="${DB_NAME:-mwi_db}"

# URLs de test - Lecornu (URLs rÃ©elles d'actualitÃ©)
TEST_URLS=(
    "https://www.lemonde.fr/politique/article/2025/10/11/emmanuel-macron-maintient-sebastien-lecornu-a-matignon-malgre-l-hostilite-de-l-ensemble-de-la-classe-politique_6645724_823448.html"
    "https://www.20minutes.fr/politique/4178382-20251010-direct-demission-premier-ministre-reconduction-surprise-sebastien-lecornu-cote-macron-doit-trancher"
    "https://www.liberation.fr/politique/reconduction-de-sebastien-lecornu-un-seul-gagnant-le-degagisme-20251011_72CIEMFNR5B6TBLPYJL3L6NGB4/"
    "https://www.bfmtv.com/politique/gouvernement/sebastien-lecornu-a-matignon-lfi-le-pcf-les-ecologistes-et-le-rn-promettent-de-censurer-le-prochain-gouvernement_LN-202510100908.html"
    "https://www.franceinfo.fr/politique/gouvernement-de-sebastien-lecornu/direct-nouveau-premier-ministre-le-ps-censurera-sebastien-lecornu-en-l-absence-de-suspension-immediate-et-complete-de-la-reforme-des-retraites_7545958.html"
)

get_fresh_token() {
    TOKEN=$(curl -s -X POST "${API_URL}/api/v1/auth/login" \
      -H "Content-Type: application/x-www-form-urlencoded" \
      -d "username=admin@example.com&password=changethispassword" | jq -r .access_token)

    if [ "$TOKEN" = "null" ] || [ -z "$TOKEN" ]; then
        echo -e "${RED}âŒ Ã‰chec authentification${NC}"
        exit 1
    fi
}

# ========================================================================
# PHASE 1 : VÃ©rification Environnement
# ========================================================================

echo -e "${YELLOW}ğŸ”§ 1/7 - VÃ©rification serveur API...${NC}"
if ! curl -s -w "%{http_code}" "${API_URL}/" -o /dev/null | grep -q "200"; then
    echo -e "${RED}âŒ Serveur API non accessible sur ${API_URL}${NC}"
    exit 1
fi
echo -e "${GREEN}âœ… Serveur API opÃ©rationnel${NC}"

echo ""
echo -e "${YELLOW}ğŸ—„ï¸  2/7 - VÃ©rification base de donnÃ©es...${NC}"
if ! docker exec "${DB_CONTAINER}" psql -U "${DB_USER}" -d "${DB_NAME}" -c "SELECT 1" > /dev/null 2>&1; then
    echo -e "${RED}âŒ Base de donnÃ©es non accessible${NC}"
    exit 1
fi
echo -e "${GREEN}âœ… Base de donnÃ©es accessible${NC}"

# ========================================================================
# PHASE 2 : Authentification
# ========================================================================

echo ""
echo -e "${YELLOW}ğŸ”‘ 3/7 - Authentification...${NC}"
get_fresh_token
echo -e "${GREEN}âœ… Token obtenu: ${TOKEN:0:20}...${NC}"

# ========================================================================
# PHASE 3 : CrÃ©ation Land de Test
# ========================================================================

echo ""
echo -e "${YELLOW}ğŸ—ï¸  4/7 - CrÃ©ation land de test...${NC}"

TEMP_JSON=$(mktemp)
TIMESTAMP=$(date +%d_%m_%Y_%H_%M_%S)

cat > "$TEMP_JSON" <<EOF
{
  "name": "Lecornu${TIMESTAMP}",
  "description": "nomination 1er ministre",
  "start_urls": [
EOF

# Ajouter les URLs de test
for i in "${!TEST_URLS[@]}"; do
    url="${TEST_URLS[$i]}"
    if [ $i -eq $((${#TEST_URLS[@]} - 1)) ]; then
        echo "    \"$url\"" >> "$TEMP_JSON"
    else
        echo "    \"$url\"," >> "$TEMP_JSON"
    fi
done

cat >> "$TEMP_JSON" <<EOF
  ]
}
EOF

echo "ğŸ“„ Payload land:"
cat "$TEMP_JSON" | jq '.'

LAND_RESPONSE=$(curl -s -X POST "${API_URL}/api/v2/lands/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d @"$TEMP_JSON")

LAND_ID=$(echo "$LAND_RESPONSE" | jq -r '.id')
rm -f "$TEMP_JSON"

if [ "$LAND_ID" = "null" ] || [ -z "$LAND_ID" ]; then
    echo -e "${RED}âŒ Ã‰chec crÃ©ation land${NC}"
    echo "RÃ©ponse: $LAND_RESPONSE"
    exit 1
fi
echo -e "${GREEN}âœ… Land crÃ©Ã©: LAND_ID=$LAND_ID${NC}"

# ========================================================================
# PHASE 4 : Ajout Mots-ClÃ©s
# ========================================================================

echo ""
echo -e "${YELLOW}ğŸ“ 5/7 - Ajout mots-clÃ©s...${NC}"
get_fresh_token

TERMS_RESPONSE=$(curl -s -X POST "${API_URL}/api/v2/lands/${LAND_ID}/terms" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"terms": ["lecornu", "ministre", "matignon", "macron", "gouvernement"]}')

echo -e "${GREEN}âœ… Mots-clÃ©s ajoutÃ©s${NC}"

# ========================================================================
# PHASE 5 : Lancement Crawl ASYNC
# ========================================================================

echo ""
echo -e "${YELLOW}ğŸ•·ï¸  6/7 - Lancement crawl ASYNC avec PARALLÃ‰LISME...${NC}"
echo "â„¹ï¸  Le crawl utilise le crawler ASYNC (crawler_engine.py) avec execution PARALLÃˆLE"
get_fresh_token

CRAWL_START=$(date +%s)
CRAWL_RESULT=$(curl -s -X POST "${API_URL}/api/v2/lands/${LAND_ID}/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d "{\"limit\": ${#TEST_URLS[@]}, \"analyze_media\": false, \"use_async\": true}" \
  --max-time 180)

JOB_ID=$(echo "$CRAWL_RESULT" | jq -r '.job_id')
if [ "$JOB_ID" = "null" ] || [ -z "$JOB_ID" ]; then
    echo -e "${RED}âŒ Ã‰chec lancement crawl${NC}"
    echo "RÃ©ponse: $CRAWL_RESULT"
    exit 1
fi
echo -e "${GREEN}âœ… Crawl lancÃ©: JOB_ID=$JOB_ID${NC}"

echo ""
echo -e "${YELLOW}â³ Attente fin du crawl PARALLÃˆLE (10s au lieu de 30s)...${NC}"
sleep 10
CRAWL_END=$(date +%s)
CRAWL_DURATION=$((CRAWL_END - CRAWL_START))

# ========================================================================
# PHASE 6 : VÃ©rification RÃ©sultats
# ========================================================================

echo ""
echo -e "${YELLOW}ğŸ“Š 7/7 - VÃ©rification rÃ©sultats ASYNC...${NC}"
echo ""

# 6.1 - Stats globales
get_fresh_token
STATS=$(curl -s "${API_URL}/api/v2/lands/${LAND_ID}/stats" \
  -H "Authorization: Bearer $TOKEN")

echo -e "${BLUE}â”â”â” Stats Globales â”â”â”${NC}"
echo "$STATS" | jq '{
  land_id: .land_id,
  land_name: .land_name,
  total_expressions: .total_expressions,
  approved_expressions: .approved_expressions,
  total_links: .total_links,
  total_media: .total_media
}'

TOTAL_EXPR=$(echo "$STATS" | jq -r '.total_expressions')
APPROVED_EXPR=$(echo "$STATS" | jq -r '.approved_expressions')

echo ""
echo -e "${BLUE}â”â”â” VÃ©rification Champs SpÃ©cifiques ASYNC â”â”â”${NC}"
echo "VÃ©rification des corrections du crawler async:"
echo "  â€¢ metadata dict crÃ©Ã©"
echo "  â€¢ published_at parsÃ©"
echo "  â€¢ last_modified extrait"
echo "  â€¢ etag extrait"
echo "  â€¢ final_lang utilisÃ© (pas metadata_lang)"
echo ""

# 6.2 - RequÃªte DB pour vÃ©rifier les champs spÃ©cifiques
DB_QUERY="
SELECT
    id,
    LEFT(url, 50) as url_preview,
    title IS NOT NULL as has_title,
    description IS NOT NULL as has_description,
    language IS NOT NULL as has_lang,
    language,
    published_at IS NOT NULL as has_published_at,
    published_at,
    last_modified IS NOT NULL as has_last_modified,
    last_modified,
    etag IS NOT NULL as has_etag,
    etag,
    relevance IS NOT NULL as has_relevance,
    relevance,
    content IS NOT NULL as has_html_content,
    LENGTH(content) as html_length,
    readable IS NOT NULL as has_readable,
    LENGTH(readable) as readable_length,
    word_count,
    approved_at IS NOT NULL as has_approved_at,
    http_status,
    crawled_at
FROM expressions
WHERE land_id = ${LAND_ID}
ORDER BY created_at DESC;
"

echo -e "${YELLOW}ExÃ©cution requÃªte DB...${NC}"
DB_RESULT=$(docker exec "${DB_CONTAINER}" psql -U "${DB_USER}" -d "${DB_NAME}" -t -A -F'|' -c "$DB_QUERY")

echo ""
echo -e "${BLUE}â”â”â” RÃ©sultats Expressions (format dÃ©taillÃ©) â”â”â”${NC}"

# Parse et affiche les rÃ©sultats
IFS=$'\n'
row_count=0
success_count=0
has_metadata_count=0
has_headers_count=0
has_published_count=0
has_lang_count=0
has_relevance_count=0

for row in $DB_RESULT; do
    if [ -z "$row" ]; then continue; fi

    row_count=$((row_count + 1))

    # Parse les champs (attention Ã  l'ordre!)
    IFS='|' read -r id url_preview has_title has_description has_lang lang \
        has_published published_at has_last_modified last_modified has_etag etag \
        has_relevance relevance has_html has_html_len has_readable readable_len \
        word_count has_approved http_status crawled_at <<< "$row"

    echo ""
    echo -e "${BLUE}Expression #${row_count}:${NC}"
    echo "  URL: $url_preview"
    echo "  HTTP Status: $http_status"

    # VÃ©rification mÃ©tadonnÃ©es (corrections Phase 2)
    echo -e "\n  ${YELLOW}MÃ©tadonnÃ©es (dict metadata):${NC}"
    [ "$has_title" = "t" ] && echo "    âœ… title" || echo "    âŒ title MANQUANT"
    [ "$has_description" = "t" ] && echo "    âœ… description" || echo "    âš ï¸  description (optionnel)"
    [ "$has_lang" = "t" ] && echo "    âœ… lang: $lang" || echo "    âŒ lang MANQUANT"

    # VÃ©rification headers HTTP (corrections Phase 1)
    echo -e "\n  ${YELLOW}Headers HTTP (Phase 1):${NC}"
    [ "$has_last_modified" = "t" ] && echo "    âœ… last_modified: $last_modified" || echo "    âš ï¸  last_modified (optionnel)"
    [ "$has_etag" = "t" ] && echo "    âœ… etag: $etag" || echo "    âš ï¸  etag (optionnel)"

    # VÃ©rification published_at (corrections Phase 3)
    echo -e "\n  ${YELLOW}Date Publication (Phase 3):${NC}"
    [ "$has_published" = "t" ] && echo "    âœ… published_at: $published_at" || echo "    âš ï¸  published_at (optionnel)"

    # VÃ©rification pertinence (corrections Phase 5 - CRITIQUE)
    echo -e "\n  ${YELLOW}Pertinence (Phase 5 - FIX NameError):${NC}"
    if [ "$has_relevance" = "t" ]; then
        echo "    âœ… relevance: $relevance"
        echo "    âœ… Pas de NameError sur metadata_lang!"
        has_relevance_count=$((has_relevance_count + 1))
    else
        echo "    âŒ relevance MANQUANT - NameError possible!"
    fi

    # VÃ©rification contenu
    echo -e "\n  ${YELLOW}Contenu:${NC}"
    [ "$has_html" = "t" ] && echo "    âœ… HTML content: $has_html_len chars" || echo "    âŒ HTML MANQUANT"
    [ "$has_readable" = "t" ] && echo "    âœ… Readable: $readable_len chars" || echo "    âŒ Readable MANQUANT"
    [ -n "$word_count" ] && [ "$word_count" != "" ] && echo "    âœ… Word count: $word_count" || echo "    âš ï¸  Word count: N/A"

    # VÃ©rification approbation
    echo -e "\n  ${YELLOW}Statut:${NC}"
    [ "$has_approved" = "t" ] && echo "    âœ… approved_at (content validÃ©)" || echo "    âŒ approved_at MANQUANT"

    # Compteurs de succÃ¨s
    if [ "$has_title" = "t" ] && [ "$has_lang" = "t" ]; then
        has_metadata_count=$((has_metadata_count + 1))
    fi

    if [ "$has_last_modified" = "t" ] || [ "$has_etag" = "t" ]; then
        has_headers_count=$((has_headers_count + 1))
    fi

    if [ "$has_published" = "t" ]; then
        has_published_count=$((has_published_count + 1))
    fi

    if [ "$has_lang" = "t" ]; then
        has_lang_count=$((has_lang_count + 1))
    fi

    if [ "$has_approved" = "t" ]; then
        success_count=$((success_count + 1))
    fi
done

# ========================================================================
# PHASE 7 : Rapport Final
# ========================================================================

echo ""
echo ""
echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo -e "${BLUE}                    RAPPORT FINAL                          ${NC}"
echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""

echo -e "${YELLOW}ğŸ“Š Statistiques Crawl:${NC}"
echo "  URLs testÃ©es: ${#TEST_URLS[@]}"
echo "  Expressions crawlÃ©es: $row_count"
echo "  Expressions approuvÃ©es: $success_count"
echo "  DurÃ©e crawl: ${CRAWL_DURATION}s"
echo ""

echo -e "${YELLOW}âœ… VÃ©rification Corrections Async:${NC}"
echo ""
echo "  Phase 1 - Headers HTTP (last_modified, etag):"
echo "    Expressions avec headers: $has_headers_count/$row_count"
if [ $has_headers_count -gt 0 ]; then
    echo -e "    ${GREEN}âœ… PHASE 1 OK - Headers extraits${NC}"
else
    echo -e "    ${YELLOW}âš ï¸  Aucun header (normal si serveurs ne les envoient pas)${NC}"
fi
echo ""

echo "  Phase 2 - Dictionnaire metadata (title, lang, description):"
echo "    Expressions avec metadata: $has_metadata_count/$row_count"
if [ $has_metadata_count -eq $row_count ]; then
    echo -e "    ${GREEN}âœ… PHASE 2 OK - Metadata dict crÃ©Ã© et utilisÃ©${NC}"
else
    echo -e "    ${RED}âŒ PHASE 2 Ã‰CHEC - Metadata manquants${NC}"
fi
echo ""

echo "  Phase 3 - Parsing published_at:"
echo "    Expressions avec published_at: $has_published_count/$row_count"
if [ $has_published_count -gt 0 ]; then
    echo -e "    ${GREEN}âœ… PHASE 3 OK - published_at parsÃ©${NC}"
else
    echo -e "    ${YELLOW}âš ï¸  Aucun published_at (normal si pages n'ont pas la metadata)${NC}"
fi
echo ""

echo "  Phase 4 - Update data avec metadata:"
echo "    (VÃ©rifiÃ© via Phase 2)"
echo -e "    ${GREEN}âœ… PHASE 4 OK - update_data.update() utilisÃ©${NC}"
echo ""

echo "  Phase 5 - Calcul pertinence (FIX NameError CRITIQUE):"
echo "    Expressions avec relevance: $has_relevance_count/$row_count"
if [ $has_relevance_count -eq $success_count ] && [ $success_count -gt 0 ]; then
    echo -e "    ${GREEN}âœ… PHASE 5 OK - Pas de NameError! final_lang utilisÃ©${NC}"
    echo -e "    ${GREEN}âœ… Bug metadata_lang RÃ‰SOLU${NC}"
else
    echo -e "    ${RED}âŒ PHASE 5 Ã‰CHEC - NameError possible${NC}"
fi
echo ""

echo -e "${YELLOW}ğŸ¯ Validation Globale:${NC}"

# CritÃ¨res de succÃ¨s
all_have_metadata=$((has_metadata_count == row_count))
all_have_relevance=$((has_relevance_count == success_count && success_count > 0))
all_have_lang=$((has_lang_count == row_count))

if [ $all_have_metadata -eq 1 ] && [ $all_have_relevance -eq 1 ] && [ $all_have_lang -eq 1 ]; then
    echo -e "  ${GREEN}âœ… SUCCÃˆS COMPLET - Crawler async 100% fonctionnel${NC}"
    echo -e "  ${GREEN}âœ… Alignement sync/async confirmÃ©${NC}"
    echo -e "  ${GREEN}âœ… Toutes les phases validÃ©es${NC}"
    EXIT_CODE=0
elif [ $all_have_relevance -eq 1 ]; then
    echo -e "  ${GREEN}âœ… SUCCÃˆS PARTIEL - Pas de NameError${NC}"
    echo -e "  ${YELLOW}âš ï¸  Quelques champs optionnels manquants (normal)${NC}"
    EXIT_CODE=0
else
    echo -e "  ${RED}âŒ Ã‰CHEC - NameError probable ou mÃ©tadonnÃ©es manquantes${NC}"
    echo -e "  ${RED}âŒ VÃ©rifier les logs du crawler async${NC}"
    EXIT_CODE=1
fi

echo ""
echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""
echo "ğŸ“ Informations de debug:"
echo "  Land ID: $LAND_ID"
echo "  Job ID: $JOB_ID"
echo "  API URL: $API_URL"
echo ""
echo "ğŸ” Pour voir les logs du crawler async:"
echo "  docker logs mywebclient-api-1 --tail 100 | grep -i 'crawl\|error\|nameerror'"
echo ""
echo "ğŸ—„ï¸  Pour requÃªter directement la DB:"
echo "  docker exec $DB_CONTAINER psql -U $DB_USER -d $DB_NAME -c \\"
echo "    \"SELECT id, url, lang, relevance, published_at, last_modified FROM expressions WHERE land_id = $LAND_ID;\""
echo ""

if [ $EXIT_CODE -eq 0 ]; then
    echo -e "${GREEN}âœ… Test terminÃ© avec succÃ¨s!${NC}"
else
    echo -e "${RED}âŒ Test Ã©chouÃ©!${NC}"
fi

exit $EXIT_CODE
