# MyWebIntelligence API - Guide Complet pour Agents

MyWebIntelligence est une API FastAPI encapsulant les fonctionnalit√©s du crawler MyWebIntelligencePython. Elle permet l'int√©gration avec MyWebClient et ouvre la voie √† un d√©ploiement SaaS scalable.

---

## üî¥ ‚ö†Ô∏è ERREUR FR√âQUENTE √Ä NE PLUS FAIRE ‚ö†Ô∏è üî¥

### **DOUBLE CRAWLER : SYNC vs ASYNC - NE PAS OUBLIER !**

Le syst√®me utilise **DEUX crawlers diff√©rents** :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚ùå ERREUR FR√âQUENTE : Modifier seulement crawler_engine.py     ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚úÖ SOLUTION : TOUJOURS modifier les DEUX crawlers !            ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  1Ô∏è‚É£  crawler_engine.py        (AsyncCrawlerEngine)            ‚îÇ
‚îÇ      ‚îî‚îÄ Utilis√© par : API directe, tests unitaires             ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  2Ô∏è‚É£  crawler_engine_sync.py   (SyncCrawlerEngine)             ‚îÇ
‚îÇ      ‚îî‚îÄ Utilis√© par : Tasks Celery (crawl en production)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Checklist OBLIGATOIRE avant tout commit :**

Quand vous modifiez la logique de crawl :

- [ ] ‚úÖ Modifier `crawler_engine.py` (async)
- [ ] ‚úÖ Modifier `crawler_engine_sync.py` (sync)
- [ ] ‚úÖ V√©rifier que les deux ont la m√™me logique
- [ ] ‚úÖ Tester avec Celery (pas seulement l'API directe)

### **Exemples de Bugs Caus√©s par Cette Erreur :**

1. **Bug du 14 octobre 2025** :
   - Champ `content` (HTML) ajout√© dans `crawler_engine.py`
   - Oubli√© dans `crawler_engine_sync.py`
   - **R√©sultat** : HTML NULL en base de donn√©es car Celery utilise la version sync

2. **Autres cas similaires** :
   - Extraction de m√©tadonn√©es (title, description, keywords)
   - Nouvelle logique de calcul de relevance
   - Modifications des champs sauvegard√©s en DB

### **Pourquoi Deux Crawlers ?**

- **Async** : Utilis√© par FastAPI (native async)
- **Sync** : Utilis√© par Celery (workers ne supportent pas async proprement)

### **Comment V√©rifier ?**

```bash
# 1. Apr√®s modification, chercher la logique dans les DEUX fichiers
grep -n "votre_modification" app/core/crawler_engine.py
grep -n "votre_modification" app/core/crawler_engine_sync.py

# 2. Tester avec Celery (pas seulement l'API)
docker logs mywebclient-celery_worker-1 --tail 50

# 3. V√©rifier en DB que les donn√©es sont bien sauvegard√©es
docker exec mywebclient-db-1 psql -U mwi_user -d mwi_db -c \
  "SELECT * FROM expressions ORDER BY created_at DESC LIMIT 1;"
```

---

## üî¥ ‚ö†Ô∏è ERREUR CATASTROPHIQUE - DATABASE INITIALIZATION ‚ö†Ô∏è üî¥

### **INCIDENT DU 17 OCTOBRE 2025 : 2 HEURES PERDUES SUR ALEMBIC**

**‚ùå CE QUI A √âT√â FAIT (MAUVAIS) :**
- Suppression d'Alembic (correct - pas de prod donc pas de migrations)
- Cr√©ation d'un script `init_db.py` externe pour cr√©er les tables
- 2 HEURES de debug sur des probl√®mes de race conditions, transactions, rollbacks...
- Complexification avec try/catch, checkfirst, isolation levels, etc.

**‚úÖ LA SOLUTION SIMPLE QUI AURAIT D√õ √äTRE FAITE D√àS LE D√âBUT :**
```python
# Dans app/main.py
@app.on_event("startup")
async def startup_event():
    """Cr√©er les tables au d√©marrage"""
    from sqlalchemy.ext.asyncio import create_async_engine
    autocommit_engine = create_async_engine(
        settings.DATABASE_URL,
        isolation_level="AUTOCOMMIT"  # ‚Üê CL√â : √©vite rollback sur erreur
    )
    try:
        async with autocommit_engine.connect() as conn:
            await conn.run_sync(Base.metadata.create_all)
        print("‚úÖ Tables cr√©√©es", flush=True)  # ‚Üê flush=True OBLIGATOIRE
    except Exception as e:
        if "already exists" in str(e):
            print("‚úÖ Tables d√©j√† existantes", flush=True)
        else:
            raise
    finally:
        await autocommit_engine.dispose()
```

### **LE√áONS APPRISES :**

1. **TOUJOURS choisir la solution la PLUS SIMPLE**
   - ‚ùå Script externe ‚Üí race conditions, gestion d'erreurs complexe
   - ‚úÖ Startup event FastAPI ‚Üí natif, simple, fonctionne

2. **AUTOCOMMIT est OBLIGATOIRE pour les DDL**
   - Sans AUTOCOMMIT : une erreur sur un index rollback TOUTES les tables
   - Avec AUTOCOMMIT : chaque CREATE TABLE est commit√© imm√©diatement
   - **R√©sultat observ√©** : "Tables d√©j√† existantes" alors qu'aucune table n'existe !

3. **Logging FastAPI : utiliser `print(flush=True)`**
   - `logger.info()` ne s'affiche PAS si le niveau de logging n'est pas configur√©
   - `print()` sans flush peut √™tre bufferis√©
   - **Solution** : `print("message", flush=True)`

4. **Ne PAS utiliser de transactions pour CREATE TABLE**
   - `engine.begin()` ‚Üí transaction ‚Üí rollback sur erreur
   - `engine.connect()` avec AUTOCOMMIT ‚Üí chaque DDL commit√©e

5. **Pas de production = Pas de migrations Alembic**
   - Supprimer Alembic compl√®tement
   - Cr√©er tables automatiquement au startup
   - Plus simple, plus maintenable

### **Checklist pour Initialisation DB :**

- [ ] ‚úÖ Cr√©er tables dans `@app.on_event("startup")` de main.py
- [ ] ‚úÖ Utiliser `isolation_level="AUTOCOMMIT"`
- [ ] ‚úÖ Utiliser `print(flush=True)` pour d√©bugger
- [ ] ‚úÖ Wrapper les erreurs "already exists" sans faire crasher
- [ ] ‚úÖ Tester avec `docker compose down -v && docker compose up -d`
- [ ] ‚úÖ V√©rifier les tables : `docker exec db psql -U user -d db -c "\dt"`

### **Erreur de Diagnostic :**

**Sympt√¥me** : "Tables d√©j√† existantes" dans les logs mais `\dt` montre 0 tables

**Cause** : Transaction rollback√©e √† cause d'une erreur sur un index, MAIS l'exception "already exists" √©tait catch√©e, donnant l'impression que tout allait bien.

**Solution** : AUTOCOMMIT pour que chaque CREATE soit commit√©e m√™me si un index √©choue ensuite.

---

## üéØ Concepts Cl√©s

### Qu'est-ce qu'un "Land" ?
Un **Land** est un projet de crawling/recherche qui contient :
- **URLs de d√©part** (`start_urls`) : Points d'entr√©e du crawl
- **Mots-cl√©s** (`words`) : Termes √† rechercher avec leurs lemmes
- **Configuration** : Langues support√©es, limites, etc.
- **R√©sultats** : Expressions (pages) et m√©dias d√©couverts

### Qu'est-ce qu'une "Expression" ?
Une **Expression** est une page web crawl√©e qui contient :
- **URL** et **contenu** de la page
- **Profondeur** (`depth`) : Distance depuis les URLs de d√©part (0 = URL initiale, 1 = lien direct, etc.)
- **Pertinence** (`relevance`) : Score de correspondance avec les mots-cl√©s
- **M√©dias** associ√©s (images, vid√©os, audio)

### Workflow Typique
1. **Cr√©er un Land** avec URLs de d√©part et mots-cl√©s
2. **Lancer le crawl** ‚Üí D√©couverte d'expressions et m√©dias
3. **Pipeline Readable** ‚Üí Extraction de contenu lisible (Mercury-like)
4. **Analyser les m√©dias** ‚Üí Extraction de m√©tadonn√©es (couleurs, dimensions, EXIF)
5. **Exporter les donn√©es** ‚Üí CSV, GEXF, JSON

## üöÄ D√©marrage Rapide

### Authentification
```bash
# Option 1‚ÄØ: depuis l‚Äôh√¥te
TOKEN=$(curl -s -X POST "http://localhost:8000/api/v1/auth/login" -H "Content-Type: application/x-www-form-urlencoded" -d "username=admin@example.com&password=changeme" | jq -r .access_token)

# Option 2‚ÄØ: via Docker Compose (container ¬´‚ÄØapi‚ÄØ¬ª)
TOKEN=$(docker compose exec api curl -s -X POST "http://localhost:8000/api/v1/auth/login" -H "Content-Type: application/x-www-form-urlencoded" -d "username=${MYWI_USERNAME:-admin@example.com}&password=${MYWI_PASSWORD:-changeme}" | jq -r .access_token)

echo "Token JWT: $TOKEN"
```

### Serveur Docker
```bash
# (Re)d√©ployer avec migrations fra√Æchement appliqu√©es
docker compose down -v
docker compose up --build -d

# V√©rifier les containers
docker ps | grep mywebintelligenceapi

# Red√©marrer si n√©cessaire
docker restart mywebintelligenceapi

# Tester que le serveur r√©pond
curl -s -w "%{http_code}" "http://localhost:8000/" -o /dev/null
```

## üèóÔ∏è Architecture de l'API

### Structure des Endpoints

#### API v1 (Legacy)
- `/api/v1/auth/` - Authentification
- `/api/v1/export/` - Export de donn√©es (CSV, GEXF)

#### API v2 (Moderne)
- `/api/v2/lands/` - Gestion des projets (lands)
- `/api/v2/lands/{id}/crawl` - Lancement de crawls
- `/api/v2/lands/{id}/readable` - Pipeline readable (extraction contenu)
- `/api/v2/lands/{id}/media-analysis-async` - Analyse des m√©dias (asynchrone)
- `/api/v2/lands/{id}/stats` - Statistiques

### Mod√®les de Donn√©es

#### Land (Projet)
```python
id: int                    # ID unique
name: str                  # Nom du projet
description: str           # Description
owner_id: int              # Propri√©taire (FK users.id)
lang: List[str]            # Langues ["fr", "en"]
start_urls: List[str]      # URLs de d√©part
crawl_status: str          # "pending", "running", "completed"
total_expressions: int     # Nombre d'expressions
total_domains: int         # Nombre de domaines
words: List[dict]          # Mots-cl√©s avec lemmes
```

#### Expression (Page crawl√©e)
```python
id: int                    # ID unique
land_id: int               # Projet parent
domain_id: int             # Domaine parent
url: str                   # URL de la page
title: str                 # Titre
content: str               # Contenu HTML
readable: str              # Contenu lisible (markdown)
depth: int                 # Profondeur de crawl
relevance: float           # Score de pertinence
quality_score: float       # Score de qualit√© (0.0-1.0) ‚ú® NOUVEAU
language: str              # Langue d√©tect√©e
word_count: int            # Nombre de mots
http_status: int           # Code HTTP (200, 404, etc.)
```

#### Media (Fichiers m√©dia)
```python
id: int                    # ID unique
expression_id: int         # Expression parent
url: str                   # URL du m√©dia
type: str                  # "img", "video", "audio"
is_processed: bool         # Analys√© ou non
width: int                 # Largeur (images)
height: int                # Hauteur (images)
file_size: int             # Taille en bytes
metadata: dict             # M√©tadonn√©es EXIF, etc.
dominant_colors: List[str] # Couleurs dominantes
```

## üÜï Mises √† jour structurelles (juillet 2025)

- **Sch√©ma SQLAlchemy r√©align√©**  
  - `domains` conserve `http_status` et `fetched_at`.  
  - `expressions` stocke d√©sormais `published_at`, `approved_at`, `validllm`, `validmodel`, `seorank`, et la langue d√©tect√©e.  
  - `words` embarque `language` et `frequency`. Une migration Alembic `006_add_legacy_crawl_columns.py` doit √™tre appliqu√©e.

- **Dictionnaire de land**  
  - `DictionaryService.populate_land_dictionary` accepte les seeds (`words`) fournis √† la cr√©ation et cr√©e automatiquement des entr√©es `Word` multilingues.  
  - Les variations g√©n√©r√©es via `get_lemma` remplissent la table `land_dictionaries` sans manipuler directement la relation ORM.

- **Service de crawl**  
  - `start_crawl_for_land` renvoie un `CrawlJobResponse` typ√© (avec `ws_channel`) et convertit les filtres `http_status` en entiers avant insertion.  
  - `CrawlerEngine` persiste la langue, approuve les expressions pertinentes et peuple les nouveaux champs de m√©tadonn√©es.
- **Migrations automatiques**  
  - Les services `api` et `celery_worker` ex√©cutent `alembic upgrade head` √† chaque d√©marrage du conteneur. Reconstruis la stack (`docker compose down -v && docker compose up --build`) apr√®s un pull pour appliquer les derniers sch√©mas.

- **Tests & environnement**  
  - Les tests de crawling n√©cessitent `pytest`, `sqlalchemy`, `aiosqlite` dans le venv.  
  - Sous Python 3.13, certaines wheels (`pydantic-core`, `asyncpg`, `pillow`) √©chouent √† la compilation ; privil√©gier Python 3.11/3.12 ou installer Rust + toolchain compatible.

## üîÑ Pipelines de Traitement

### 1. Pipeline de Crawl
```
Start URLs ‚Üí Crawler ‚Üí Pages ‚Üí Content Extraction ‚Üí Expressions
                          ‚Üì
                     Media Detection ‚Üí Media Records
```

**Endpoint:** `POST /api/v2/lands/{id}/crawl`
```json
{
  "limit": 10,              // Nombre max de pages
  "depth": 2,               // Profondeur max
  "analyze_media": true     // Analyser les m√©dias
}
```

### 2. Pipeline d'Analyse Media
```
Expressions ‚Üí Media URLs ‚Üí Download ‚Üí Analysis ‚Üí Metadata Storage
                               ‚Üì
                         PIL/OpenCV ‚Üí Colors, Dimensions, EXIF
```

**Endpoint:** `POST /api/v2/lands/{id}/media-analysis-async`
```json
{
  "depth": 1,               // Profondeur max des expressions √† analyser (0=URLs initiales, 1=liens directs, etc.)
  "minrel": 0.5             // Score de pertinence minimum des expressions
}
```

**IMPORTANT:** `depth` ne limite PAS le nombre d'unit√©s/m√©dias √† analyser, mais la profondeur des expressions source !
- `depth: 0` = Analyser uniquement les m√©dias des URLs de d√©part
- `depth: 1` = Inclure aussi les m√©dias des pages li√©es directement 
- `depth: 999` = Analyser tous les m√©dias sans limite de profondeur

**STRAT√âGIE de LIMITATION:** Pour limiter le nombre de m√©dias analys√©s, utiliser `minrel` (pertinence) :
- `minrel: 0.0` = Toutes les expressions
- `minrel: 1.0` = Expressions moyennement pertinentes  
- `minrel: 3.0` = Expressions tr√®s pertinentes seulement (recommand√© pour tests)
- `minrel: 5.0` = Expressions extr√™mement pertinentes

### 3. Pipeline Readable (Nouveau)
```
Expressions ‚Üí Content Extraction ‚Üí Readable Content ‚Üí Text Processing ‚Üí Paragraphs
                     ‚Üì
            Mercury/Trafilatura ‚Üí Clean Text ‚Üí LLM Validation ‚Üí Storage
```

**Endpoint:** `POST /api/v2/lands/{id}/readable`
```json
{
  "limit": 50,              // Nombre max d'expressions √† traiter
  "depth": 1,               // Profondeur max des expressions
  "merge_strategy": "smart_merge",  // "mercury_priority", "preserve_existing"
  "enable_llm": false,      // Validation LLM (optionnel)
  "batch_size": 10,         // Expressions par batch
  "max_concurrent": 5       // Batches concurrents
}
```

**IMPORTANT:** Le pipeline readable transforme le contenu HTML brut des expressions en contenu lisible et structur√© :
- **Extraction intelligente** : Utilise Trafilatura puis Mercury fallback
- **Nettoyage** : Supprime le markup, garde le contenu principal
- **Validation LLM** : Optionnelle, am√©liore la qualit√© du contenu
- **Segmentation** : D√©coupe en paragraphes pour l'embedding

### 4. Pipeline LLM Validation (Int√©gr√©) ‚úÖ
```
Expressions ‚Üí OpenRouter API ‚Üí Relevance Check ‚Üí Database Update
                     ‚Üì
              "oui"/"non" ‚Üí valid_llm, valid_model ‚Üí Relevance=0 si non pertinent
```

**Int√©gration dans les pipelines existants :**
- **Crawl avec LLM** : `POST /api/v2/lands/{id}/crawl` avec `"enable_llm": true`
- **Readable avec LLM** : `POST /api/v2/lands/{id}/readable` avec `"enable_llm": true`

**Configuration OpenRouter requise :**
```bash
export OPENROUTER_ENABLED=True
export OPENROUTER_API_KEY=sk-or-v1-your-key-here
export OPENROUTER_MODEL=anthropic/claude-3.5-sonnet
```

**Exemples d'usage :**
```bash
# Crawl avec validation LLM
curl -X POST "http://localhost:8000/api/v2/lands/36/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"limit": 5, "enable_llm": true}'

# Readable avec validation LLM  
curl -X POST "http://localhost:8000/api/v2/lands/36/readable" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"limit": 3, "enable_llm": true}'
```

**R√©sultats stock√©s :**
- `valid_llm` : "oui" (pertinent) ou "non" (non pertinent)
- `valid_model` : Mod√®le utilis√© (ex: "anthropic/claude-3.5-sonnet")
- `relevance` : Mis √† 0 si expression jug√©e non pertinente

---

## üèÜ Quality Score System (Nouveau - Octobre 2025) ‚úÖ

### ‚ö†Ô∏è DOUBLE CRAWLER : Impl√©mentation Compl√®te

**‚úÖ IMPL√âMENT√â DANS LES DEUX CRAWLERS :**
- ‚úÖ `crawler_engine.py` (AsyncCrawlerEngine) - lignes 269-317
- ‚úÖ `crawler_engine_sync.py` (SyncCrawlerEngine) - lignes 341-389
- ‚úÖ **Parit√© parfaite** : M√™me logique dans les deux crawlers

### üìä Vue d'Ensemble

Le **Quality Score** est un indicateur de qualit√© automatique pour chaque expression (page crawl√©e), calcul√© √† partir de m√©tadonn√©es existantes. Score entre **0.0** (tr√®s faible) et **1.0** (excellent).

```
Quality Score = Œ£ (Bloc_i √ó Poids_i)

5 Blocs Heuristiques :
1Ô∏è‚É£  Access (30%)      ‚Üí HTTP status, content-type
2Ô∏è‚É£  Structure (15%)   ‚Üí Title, description, keywords, canonical
3Ô∏è‚É£  Richness (25%)    ‚Üí Word count, ratio texte/HTML, reading time
4Ô∏è‚É£  Coherence (20%)   ‚Üí Langue, relevance, fra√Æcheur
5Ô∏è‚É£  Integrity (10%)   ‚Üí LLM validation, pipeline complet
```

**Cat√©gories de Qualit√© :**
- `0.8-1.0` : **Excellent** ‚≠ê (Contenu riche, bien structur√©)
- `0.6-0.8` : **Bon** ‚úÖ (Contenu acceptable)
- `0.4-0.6` : **Moyen** ‚ö†Ô∏è (Contenu limit√©)
- `0.2-0.4` : **Faible** ‚ùå (Tr√®s pauvre)
- `0.0-0.2` : **Tr√®s faible** ‚ùå‚ùå (Erreur d'acc√®s)

### üéØ Caract√©ristiques

- ‚úÖ **100% d√©terministe** : Pas de ML/LLM, reproductible
- ‚úÖ **Gratuit** : Pas d'appels API externes
- ‚úÖ **Rapide** : <10ms par expression
- ‚úÖ **Transparent** : Heuristiques document√©es
- ‚úÖ **Configurable** : Poids ajustables via settings

### ‚öôÔ∏è Configuration

Dans `.env` ou `app/config.py` :
```python
# Master switch (activ√© par d√©faut)
ENABLE_QUALITY_SCORING=true

# Poids des 5 blocs (doivent sommer √† 1.0)
QUALITY_WEIGHT_ACCESS=0.30      # Acc√®s
QUALITY_WEIGHT_STRUCTURE=0.15   # Structure HTML/SEO
QUALITY_WEIGHT_RICHNESS=0.25    # Richesse contenu
QUALITY_WEIGHT_COHERENCE=0.20   # Coh√©rence land/langue
QUALITY_WEIGHT_INTEGRITY=0.10   # Int√©grit√© pipeline
```

### üöÄ Usage Automatique

Le `quality_score` est **calcul√© automatiquement** lors du crawl :

```bash
# Via API
curl -X POST "http://localhost:8000/api/v2/lands/15/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"limit": 100}'

# Via Celery
from app.core.celery_app import crawl_land_task
crawl_land_task.delay(land_id=15, limit=100)
```

Le champ `quality_score` est automatiquement rempli dans la DB pour chaque expression crawl√©e.

### üîÑ Reprocessing Historique

Pour recalculer les quality_scores sur expressions existantes :

```bash
# Dry-run (simulation)
docker exec mywebintelligenceapi python -m app.scripts.reprocess_quality_scores --dry-run

# Reprocess toutes les expressions sans quality_score
docker exec mywebintelligenceapi python -m app.scripts.reprocess_quality_scores

# Reprocess un land sp√©cifique
docker exec mywebintelligenceapi python -m app.scripts.reprocess_quality_scores --land-id 15

# Limiter le nombre d'expressions
docker exec mywebintelligenceapi python -m app.scripts.reprocess_quality_scores --limit 1000

# Forcer le recalcul m√™me si quality_score existe
docker exec mywebintelligenceapi python -m app.scripts.reprocess_quality_scores --force
```

**Exemple de sortie :**
```
============================================================
REPROCESSING SUMMARY
============================================================
Total candidates:     70
Processed:            70
Updated:              70
Errors:               0
Duration:             9.5s

Quality Distribution:
  Excellent      :   25 ( 35.7%)
  Bon            :   45 ( 64.3%)
  Moyen          :    0 (  0.0%)
  Faible         :    0 (  0.0%)
  Tr√®s faible    :    0 (  0.0%)
============================================================
```

### üîç Requ√™tes SQL Utiles

**Statistiques globales :**
```sql
SELECT
  COUNT(*) as total,
  COUNT(quality_score) as with_quality,
  ROUND(AVG(quality_score)::numeric, 3) as avg_score
FROM expressions;
```

**Distribution par cat√©gorie :**
```sql
SELECT
  CASE
    WHEN quality_score >= 0.8 THEN 'Excellent'
    WHEN quality_score >= 0.6 THEN 'Bon'
    WHEN quality_score >= 0.4 THEN 'Moyen'
    WHEN quality_score >= 0.2 THEN 'Faible'
    ELSE 'Tr√®s faible'
  END as category,
  COUNT(*) as count,
  ROUND(AVG(quality_score)::numeric, 3) as avg_score
FROM expressions
WHERE quality_score IS NOT NULL
GROUP BY category
ORDER BY avg_score DESC;
```

**Top 10 meilleures expressions :**
```sql
SELECT id, url, quality_score, word_count, relevance
FROM expressions
WHERE quality_score IS NOT NULL
ORDER BY quality_score DESC
LIMIT 10;
```

### üìö Fichiers du Syst√®me

| Fichier | Description |
|---------|-------------|
| `app/services/quality_scorer.py` | Service de calcul (5 blocs) |
| `app/core/crawler_engine.py` | Int√©gration ASYNC (lignes 269-317) |
| `app/core/crawler_engine_sync.py` | Int√©gration SYNC (lignes 341-389) |
| `app/scripts/reprocess_quality_scores.py` | Script de reprocessing |
| `tests/unit/test_quality_scorer.py` | 33 tests unitaires ‚úÖ |
| `tests/data/quality_truth_table.json` | 20 cas de validation |
| `.claude/docs/QUALITY_SCORE_GUIDE.md` | Documentation compl√®te (500+ lignes) |

### üß™ Tests

```bash
# Tests unitaires (33 tests)
docker exec mywebintelligenceapi pytest tests/unit/test_quality_scorer.py -v

# Validation truth table
docker exec mywebintelligenceapi pytest tests/unit/test_quality_scorer.py::TestTruthTable -v
```

### üéì Documentation Compl√®te

Pour les d√©tails complets (heuristiques, tuning, troubleshooting) :
**Voir** : `.claude/docs/QUALITY_SCORE_GUIDE.md`

---

### 5. Pipeline d'Export
```
Data Selection ‚Üí Format Conversion ‚Üí Response
     ‚Üì
CSV/GEXF/JSON ‚Üí Compressed ‚Üí Download
```

**Endpoint:** `POST /api/v1/export/direct`
```json
{
  "land_id": 36,
  "export_type": "mediacsv", // "pagecsv", "nodecsv", etc.
  "limit": 100
}
```

## üóÑÔ∏è Base de Donn√©es

### Tables Principales
- `users` - Utilisateurs
- `lands` - Projets de crawl
- `domains` - Domaines web
- `expressions` - Pages crawl√©es
- `media` - Fichiers m√©dia
- `paragraphs` - Contenu segment√©
- `crawl_jobs` - Jobs Celery

### Relations Cl√©s
```
users (1) ‚Üí (n) lands
lands (1) ‚Üí (n) domains
lands (1) ‚Üí (n) expressions
expressions (1) ‚Üí (n) media
expressions (1) ‚Üí (n) paragraphs
```

## üß™ Tests Rapides

### ‚ö†Ô∏è PROBL√àME DE PERSISTENCE DU TOKEN

Le token JWT ne persiste pas car `export TOKEN` dans un **sous-shell Docker** ne remonte pas au shell parent. Voici **3 solutions** :

---

### ‚úÖ **Solution 1 : Script complet dans Docker** (recommand√© pour les tests)

Ex√©cuter tout le workflow dans un seul appel Docker pour que le token reste en m√©moire :

```bash
docker compose exec mywebintelligenceapi bash -c '
  # 1. Authentification
  TOKEN=$(curl -s -X POST "http://localhost:8000/api/v1/auth/login" \
    -H "Content-Type: application/x-www-form-urlencoded" \
    -d "username=admin@example.com&password=changethispassword" \
    | jq -r ".access_token")

  echo "‚úÖ Token obtenu : ${TOKEN:0:20}..."

  # 2. Cr√©er le land
  LAND_ID=$(curl -s -X POST "http://localhost:8000/api/v2/lands/" \
    -H "Authorization: Bearer $TOKEN" \
    -H "Content-Type: application/json" \
    -d "{\"name\":\"Lecornu$(date +%s)\",\"description\":\"Test gilets jaunes\",\"words\":[\"lecornu\"]}" \
    | jq -r ".id // empty")

  if [ -z "$LAND_ID" ]; then
    echo "‚ùå Impossible de cr√©er le land"
    exit 1
  fi

  echo "‚úÖ Land cr√©√© : LAND_ID=${LAND_ID}"

  # 3. Ajouter les URLs
  curl -s -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/urls" \
    -H "Authorization: Bearer $TOKEN" \
    -H "Content-Type: application/json" \
    --data "$(jq -Rs "{urls: split(\"\n\") | map(select(length>0))}" /app/scripts/data/lecornu.txt)" \
    | jq "."

  echo "‚úÖ URLs ajout√©es au land ${LAND_ID}"
'
```

---

### ‚úÖ **Solution 2 : Appels depuis l'h√¥te** (si API accessible sur localhost)

Stocker le token dans un fichier temporaire pour le r√©utiliser :

```bash
# 1. Authentification (sauvegarder le token dans un fichier)
curl -s -X POST "http://localhost:8000/api/v1/auth/login" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=admin@example.com&password=changethispassword" \
  | jq -r '.access_token' > /tmp/mywebintel_token.txt

export TOKEN=$(cat /tmp/mywebintel_token.txt)
echo "‚úÖ Token obtenu : ${TOKEN:0:20}..."

# 2. Cr√©er le land
LAND_ID=$(curl -s -X POST "http://localhost:8000/api/v2/lands/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"name":"Lecornu202535","description":"testprojet gilets jaunes","words":["lecornu"]}' \
  | jq -r '.id // empty')

if [ -z "$LAND_ID" ]; then
  echo "‚ùå Impossible de cr√©er le land. V√©rifiez les logs ou les permissions."
  exit 1
else
  echo "‚úÖ Land cr√©√© : LAND_ID=${LAND_ID}"

  # 3. Ajouter la liste d'URLs
  curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/urls" \
    -H "Authorization: Bearer $TOKEN" \
    -H "Content-Type: application/json" \
    --data "$(jq -Rs '{urls: split("\n") | map(select(length>0))}' MyWebIntelligenceAPI/scripts/data/lecornu.txt)"
fi
```

---

### ‚úÖ **Solution 3 : Session interactive dans le container** (pour tests manuels)

Entrer directement dans le container pour garder le token en m√©moire :

```bash
# 1. Entrer dans le container
docker compose exec mywebintelligenceapi bash

# 2. Dans le container, authentification + export
TOKEN=$(curl -s -X POST "http://localhost:8000/api/v1/auth/login" -H "Content-Type: application/x-www-form-urlencoded" -d "username=admin@example.com&password=changethispassword" | jq -r '.access_token')

export TOKEN
echo "‚úÖ Token : ${TOKEN:0:20}..."

# 3. Maintenant toutes vos commandes curl peuvent utiliser $TOKEN
# Cr√©er le land
LAND_ID=$(curl -s -X POST "http://localhost:8000/api/v2/lands/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"name":"Lecornu202540","description":"Test gilets jaunes","words":["lecornu"]}' \
  | jq -r '.id')

echo "‚úÖ Land cr√©√© : $LAND_ID"

# Ajouter les URLs
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/urls" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  --data "$(jq -Rs '{urls: split("\n") | map(select(length>0))}' /app/scripts/data/lecornu.txt)"

# Pour sortir du container :
# exit
```

### 1. Lister les Lands
```bash
curl -X GET "http://localhost:8000/api/v2/lands/?page=1&page_size=20" \
  -H "Authorization: Bearer $TOKEN"
```

### 2. D√©tails d'un Land
```bash
curl -X GET "http://localhost:8000/api/v2/lands/${LAND_ID}" \
  -H "Authorization: Bearer $TOKEN"
```

### 3. Statistiques d'un Land
```bash
curl -X GET "http://localhost:8000/api/v2/lands/${LAND_ID}/stats" \
  -H "Authorization: Bearer $TOKEN"
```

### 4. Lancer un Crawl avec Analyse Media
```bash
curl -X POST "http://localhost:8000/api/v2/lands/7/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"analyze_media": false, "limit": 25, "llm_validation": false}'
```

### 5. Analyser les M√©dias (ASYNC)
```bash
# Analyser TOUS les m√©dias (toutes profondeurs, toute pertinence)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"depth": 0, "minrel": 3.0}'

# Analyser uniquement les m√©dias des URLs de d√©part (depth=0)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"depth": 0, "minrel": 0.0}'

# Analyser avec filtre de pertinence (expressions tr√®s pertinentes seulement)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"minrel": 3.0}'

# TEST RAPIDE - Analyser seulement les plus pertinents (recommand√©)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"minrel": 3.0}'
```

### 6. Pipeline Readable - Extraction de Contenu (‚úÖ FONCTIONNEL)
```bash
# üéØ TEST DIRECT CELERY (recommand√© pour v√©rifier que √ßa marche)
docker-compose exec celery_worker python -c "
from app.tasks.readable_working_task import readable_working_task
result = readable_working_task.delay(land_id=1, job_id=999, limit=2)
print(f'Task ID: {result.id}')
import time; time.sleep(15)
print(f'Result: {result.get()}')"

# üìã PIPELINE COMPLET (via API - maintenant fonctionnel!)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/readable" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"limit": 10}' \
  --max-time 60

# üîß LOGS EN TEMPS R√âEL
docker-compose logs celery_worker --tail=20 -f

# ‚úÖ R√âSULTATS ATTENDUS:
# - Content extraction avec Trafilatura + fallback Archive.org
# - Processing r√©ussi avec logs d√©taill√©s
# - Extraction de 3000+ caract√®res de contenu readable
# - Dur√©e: 15-20 secondes pour 2 URLs
# - Status: completed avec statistiques d√©taill√©es

# üìä STATISTIQUES TYPIQUES:
# - processed: 2 URLs
# - successful: 1-2 selon le contenu disponible  
# - errors: 0-1 (example.com peut √©chouer)
# - readable_length: 3000+ caract√®res extraits
# - source: "archive_org" (fallback utilis√©)
```

## üêõ D√©buggage Fr√©quent

### Erreurs SQL
- **Probl√®me:** `should be explicitly declared as text()`
- **Solution:** Ajouter `from sqlalchemy import text` et wrapper les requ√™tes avec `text()`

### Erreurs de Mod√®le
- **Probl√®me:** `'Land' object has no attribute 'user_id'`
- **Solution:** Utiliser `owner_id` au lieu de `user_id`

### Timeouts & Freeze
- **Probl√®me:** L'analyse media timeout ou freeze (pas de r√©ponse)
- **Cause:** Traitement lourd des images (PIL/sklearn) sur trop de m√©dias
- **Solutions √©prouv√©es:** 
  - **PRIORIT√â 1:** Utiliser `minrel: 3.0` ou plus pour r√©duire drastiquement le nombre d'expressions
  - Utiliser `depth: 0` pour analyser seulement les URLs initiales
  - Ajouter `--max-time 120` √† curl pour √©viter les timeouts infinis
  - V√©rifier les logs Celery: `docker logs mywebintelligenceapi_celery_1`
  - Red√©marrer le worker: `docker restart mywebintelligenceapi_celery_1`

### R√©sultats d'Analyse Typiques
- **Expressions totales:** ~1000+
- **Avec minrel=3.0:** ~2-5 expressions (filtrage tr√®s efficace)
- **M√©dias analys√©s:** 7-50 selon la pertinence
- **√âchecs fr√©quents:** Images 404, SVG non support√©s, URLs de tracking
- **Temps de traitement:** 60-120s avec filtrage, timeout sans filtrage

### üö® Probl√®me URLs de M√©dias Incorrectes
- **Probl√®me majeur:** URLs de m√©dias g√©n√©r√©es incorrectement durant le crawl
- **Causes principales:**
  - Proxies WordPress (i0.wp.com, i1.wp.com) qui ne fonctionnent plus
  - URLs relatives mal r√©solues
  - Manque de validation d'URLs
  - Attributs alternatifs (srcset, data-src) ignor√©s
- **Solution:** Nouveau syst√®me de nettoyage d'URLs dans MediaProcessor
- **Impact:** R√©duction drastique des erreurs 404 lors de l'analyse

### üö® PROBL√àME CRITIQUE : Dictionary Starvation - R√âSOLU ‚úÖ
- **Cause racine:** Lands cr√©√©s sans dictionnaires de mots-cl√©s peupl√©s
- **Sympt√¥mes:** 
  - Toutes les expressions ont pertinence = 0
  - Crawler suit tous les liens sans discrimination
  - Explosion du nombre d'expressions (1831 pour 10 URLs)
- **Impact:** Syst√®me de pertinence compl√®tement cass√©
- **Solution impl√©ment√©e:** 
  - Auto-population des dictionnaires lors de la cr√©ation des lands (`crud_land.py`)
  - Service `DictionaryService` pour g√©rer les dictionnaires
  - Endpoints `/populate-dictionary` et `/dictionary-stats` pour diagnostiquer
- **Code impact√©:** `text_processing.expression_relevance()` retourne 0 si dictionnaire vide

### üîß Pipeline de Crawl - Logique des Timestamps ‚úÖ

**LOGIQUE STRICTE DES DATES (CRITIQUE) :**

1. **`created_at`** ‚Üí Quand l'expression est **ajout√©e en base** (d√©couverte de l'URL)
   - Rempli automatiquement lors de l'INSERT
   - Permet de suivre l'ordre de d√©couverte des URLs

2. **`crawled_at`** ‚Üí Quand le **contenu HTTP a √©t√© r√©cup√©r√©** (fetch HTTP r√©ussi)
   - Rempli apr√®s un GET HTTP r√©ussi avec code HTTP valide
   - NULL si l'URL n'a jamais √©t√© fetch√©e

3. **`approved_at`** ‚Üí Quand le crawler a **trait√© la r√©ponse** (m√™me si erreur)
   - ‚ö†Ô∏è **CRIT√àRE DE S√âLECTION PRINCIPAL** : `approved_at IS NULL` = candidats au crawl
   - Rempli syst√©matiquement apr√®s traitement (succ√®s ou √©chec)
   - Marque l'expression comme "trait√©e par le crawler"

4. **`readable_at`** ‚Üí Quand le **contenu readable** a √©t√© extrait et enregistr√©
   - Rempli apr√®s extraction r√©ussie (Trafilatura/Mercury)
   - NULL si pas encore de contenu lisible

5. **`updated_at`** ‚Üí Quand le contenu **readable a √©t√© modifi√©**
   - Mis √† jour automatiquement √† chaque modification de `readable`
   - Permet de tracker les re-extractions

**HTTP_STATUS - R√àGLE STRICTE :**

- **TOUJOURS** un code HTTP valide (200, 404, 500, etc.)
- **OU** `000` pour erreur inconnue non-HTTP (timeout, DNS, etc.)
- **JAMAIS** NULL apr√®s traitement

**WORKFLOW TYPIQUE :**

```text
1. D√©couverte URL     ‚Üí created_at = NOW, approved_at = NULL, http_status = NULL
2. Fetch HTTP         ‚Üí crawled_at = NOW, http_status = 200 (ou 404, etc.)
3. Traitement r√©ponse ‚Üí approved_at = NOW
4. Extraction readable ‚Üí readable_at = NOW, updated_at = NOW
5. Modification readable ‚Üí updated_at = NOW (updated_at > readable_at)
```

**REQU√äTE DE S√âLECTION DES CANDIDATS AU CRAWL :**

```sql
SELECT * FROM expressions
WHERE land_id = ?
  AND approved_at IS NULL  -- ‚ö†Ô∏è Cl√© principale !
  AND depth <= ?           -- Filtre optionnel de profondeur
ORDER BY depth ASC, created_at ASC
LIMIT ?
```

**Endpoints de diagnostic du pipeline:**
- `GET /api/v2/lands/{id}/pipeline-stats` - Statistiques compl√®tes du pipeline
- `POST /api/v2/lands/{id}/fix-pipeline` - R√©pare les incoh√©rences de dates

### Container Docker
- **Probl√®me:** Changements de code non pris en compte
- **Solution:** `docker restart mywebintelligenceapi`

### Logs d'Analyse M√©dia
- **IMPORTANT:** L'analyse m√©dia est **ASYNCHRONE** (avec Celery)
- **Logs √† surveiller:** `docker logs mywebclient-celery_worker-1 -f`
- **Signaux d'activit√©:** 
  - `sklearn.base.py:1152: ConvergenceWarning` = Clustering couleurs dominantes
  - `PIL/Image.py:975: UserWarning` = Traitement d'images avec transparence
  - Task termin√© avec succ√®s dans les logs Celery

## üìä Donn√©es de Test

### Land 36 "giletsjaunes"
- **ID:** 36
- **URLs:** Blogs gilets jaunes (over-blog.com, etc.)
- **Contenu:** Articles sur le mouvement
- **M√©dias:** ~850 images selon les stats mock√©es

### Autres Lands
- **37, 38, 39:** Lands de test avec URLs example.com
- **40:** Land basique avec example.org
- **41:** Autre land gilets jaunes

## üîß Technologies

### Backend
- **FastAPI** - Framework web moderne avec validation automatique
- **SQLAlchemy 2.0** - ORM async avec mod√®les d√©claratifs
- **PostgreSQL 15+** - Base de donn√©es relationnelle
- **Celery** - T√¢ches asynchrones distribu√©es 
- **Redis** - Broker Celery et cache

### Analyse Media
- **PIL/Pillow** - Traitement d'images (dimensions, format, EXIF)
- **OpenCV** - Vision par ordinateur avanc√©e
- **scikit-learn** - Machine learning (clustering couleurs dominantes)
- **httpx** - Client HTTP asynchrone pour t√©l√©chargement

### Architecture Async
- **AsyncSession** - Connexions DB non-bloquantes
- **async/await** - Gestion asynchrone des t√¢ches lourdes
- **WebSocket** - Suivi temps r√©el des jobs

### Containerisation
- **Docker Compose** - Orchestration multi-services
- **Services:** API, Celery Worker, PostgreSQL, Redis, Flower (monitoring)
- **Volumes persistants** - Donn√©es et logs
- **Network isolation** - S√©curit√© inter-services

### Installation & Configuration
```bash
# Installation compl√®te avec Docker
git clone <repository-url>
cd MyWebIntelligenceAPI
cp .env.example .env           # Configurer DATABASE_URL, REDIS_URL, SECRET_KEY
docker-compose up -d           # Lancer tous les services
docker-compose exec api alembic upgrade head    # Migrations DB

# Acc√®s services
# API: http://localhost:8000
# Docs: http://localhost:8000/docs  
# Flower: http://localhost:5555
# Prometheus: http://localhost:9090
# Grafana: http://localhost:3001
```

### Variables d'Environnement Critiques
```bash
# Base de donn√©es
DATABASE_URL=postgresql://user:pass@postgres:5432/mywebintelligence

# Cache/Queue  
REDIS_URL=redis://redis:6379

# S√©curit√©
SECRET_KEY=<g√©n√©rer-cl√©-al√©atoire-64-chars>
FIRST_SUPERUSER_EMAIL=admin@example.com
FIRST_SUPERUSER_PASSWORD=changethispassword

# API Externe (optionnel)
OPENROUTER_API_KEY=<pour-analyse-s√©mantique>
```

---

## üöÄ TEST RAPIDE COMPLET - SCRIPT AUTOMATIS√â

### ‚úÖ Script de Test Crawl SYNC (RECOMMAND√â - 1 minute)

**Localisation**: `MyWebIntelligenceAPI/tests/test-crawl-simple.sh`

Ce script teste le **crawl synchrone** des 5 URLs Lecornu **sans les fonctionnalit√©s async buggu√©es**.

```bash
#!/bin/bash
# Test SIMPLE crawl sync - 5 URLs Lecornu
# Sans media analysis async ni readable pipeline

get_fresh_token() {
    TOKEN=$(curl -s -X POST "http://localhost:8000/api/v1/auth/login" \
      -H "Content-Type: application/x-www-form-urlencoded" \
      -d "username=admin@example.com&password=changethispassword" | jq -r .access_token)
    if [ "$TOKEN" = "null" ] || [ -z "$TOKEN" ]; then
        echo "‚ùå √âchec authentification"
        exit 1
    fi
}

echo "üîß 1/5 - V√©rification serveur..."
if ! curl -s -w "%{http_code}" "http://localhost:8000/" -o /dev/null | grep -q "200"; then
    echo "‚ùå Serveur API non accessible"
    exit 1
fi

echo "üîë 2/5 - Authentification..."
get_fresh_token
echo "‚úÖ Token: ${TOKEN:0:20}..."

echo "üèóÔ∏è 3/5 - Cr√©ation land..."
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
LECORNU_FILE="${SCRIPT_DIR}/../scripts/data/lecornu.txt"

if [ ! -f "$LECORNU_FILE" ]; then
    echo "‚ùå Fichier lecornu.txt non trouv√©"
    exit 1
fi

TEMP_JSON=$(mktemp)
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
cat > "$TEMP_JSON" <<EOF
{
  "name": "test_lecornu_${TIMESTAMP}",
  "description": "Test crawl sync Lecornu - ${TIMESTAMP}",
  "start_urls": [
EOF

head -n 5 "$LECORNU_FILE" | while IFS= read -r url; do
    if [ -n "$url" ]; then
        echo "    \"$url\"," >> "$TEMP_JSON"
    fi
done

sed -i '' '$ s/,$//' "$TEMP_JSON" 2>/dev/null || sed -i '$ s/,$//' "$TEMP_JSON"
cat >> "$TEMP_JSON" <<EOF

  ]
}
EOF

LAND_ID=$(curl -s -X POST "http://localhost:8000/api/v2/lands/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d @"$TEMP_JSON" | jq -r '.id')
rm -f "$TEMP_JSON"

if [ "$LAND_ID" = "null" ] || [ -z "$LAND_ID" ]; then
    echo "‚ùå √âchec cr√©ation land"
    exit 1
fi
echo "‚úÖ Land cr√©√©: LAND_ID=$LAND_ID"

echo "üìù 4/5 - Ajout mots-cl√©s..."
get_fresh_token
curl -s -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/terms" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"terms": ["lecornu", "sebastien", "macron", "matignon"]}' > /dev/null

echo "üï∑Ô∏è 5/5 - Lancement crawl SYNC..."
get_fresh_token
CRAWL_RESULT=$(curl -s -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"limit": 5}' --max-time 120)

JOB_ID=$(echo "$CRAWL_RESULT" | jq -r '.job_id')
if [ "$JOB_ID" = "null" ] || [ -z "$JOB_ID" ]; then
    echo "‚ùå √âchec crawl: $CRAWL_RESULT"
    exit 1
fi
echo "‚úÖ Crawl lanc√©: JOB_ID=$JOB_ID"

echo ""
echo "‚è≥ Attente fin du crawl (60s)..."
sleep 60

echo ""
echo "üìä V√©rification r√©sultats..."
get_fresh_token
STATS=$(curl -s "http://localhost:8000/api/v2/lands/${LAND_ID}/stats" \
  -H "Authorization: Bearer $TOKEN")

echo ""
echo "üéØ R√âSULTATS:"
echo "$STATS" | jq '{
  land_id: .land_id,
  land_name: .land_name,
  total_expressions: .total_expressions,
  approved_expressions: .approved_expressions,
  total_links: .total_links,
  total_media: .total_media
}'

echo ""
echo "‚úÖ Test termin√©!"
echo "Land ID: $LAND_ID"
echo "Job ID: $JOB_ID"
```

**R√©sultats Attendus:**
```
‚úÖ URLs Processed: 5
‚úÖ Errors: 0
‚úÖ Duration: ~50 seconds
‚úÖ Content extrait: ~50,000 caract√®res
```

### Utilisation
```bash
# Depuis la racine du projet
./MyWebIntelligenceAPI/tests/test-crawl-simple.sh

# V√©rifier les logs Celery
docker logs mywebclient-celery_worker-1 --tail=50 | grep "CRAWL COMPLETED" -A 5
```

---

### ‚ö†Ô∏è Script Complet avec Async (D√âPR√âCI√â - contient des bugs)

Le script original avec analyse m√©dia async et pipeline readable contient des bugs asyncio et n'est **pas recommand√©** pour le moment.

**Probl√®mes connus:**
- ‚ùå `RuntimeError: Task got Future attached to a different loop` dans media analysis async
- ‚ùå Pipeline Readable utilise des URLs de test hardcod√©es (example.com, httpbin.org)
- ‚ùå Erreurs `InterfaceError: another operation is in progress` dans les batch tasks

**Script disponible**: `MyWebIntelligenceAPI/tests/test-crawl.sh` (pour r√©f√©rence uniquement)

```

## üêõ CORRECTIONS CRITIQUES AGENTS - Le√ßons Apprises

### ‚ùå **Erreurs Fr√©quentes √† √âviter**

#### 1. **Bug `metadata_lang` non d√©fini** (R√âSOLU - 2025-10-17)
- **Probl√®me** : `name 'metadata_lang' is not defined` lors du crawl
- **Cause** : Variable renomm√©e de `metadata_lang` ‚Üí `final_lang` mais usage ancien non mis √† jour
- **Fichier** : `/app/app/core/crawler_engine_sync.py:251,256`
- **Fix** : Remplacer `metadata_lang` par `final_lang` dans l'appel √† `expression_relevance()`
- **Impact** : 100% des URLs √©chouaient avant le fix

**Code corrig√©:**
```python
# AVANT (buggu√©)
relevance = asyncio.run(
    text_processing.expression_relevance(land_dict, temp_expr, metadata_lang or "fr")
)

# APR√àS (corrig√©)
relevance = asyncio.run(
    text_processing.expression_relevance(land_dict, temp_expr, final_lang or "fr")
)
```

#### 2. **Bug job_id** (R√âSOLU)
- **Probl√®me** : `job_id should be a valid integer [input_value=None]`
- **Cause** : `/app/api/v2/endpoints/lands_v2.py:582` cherchait `"id"` au lieu de `"job_id"`
- **Fix** : `job_payload.get("id")` ‚Üí `job_payload.get("job_id")`

#### 3. **Tokens JWT Expirent Rapidement** ‚ö†Ô∏è
- **Probl√®me** : `Could not validate credentials` apr√®s quelques minutes
- **Solution** : Fonction `get_fresh_token()` avant chaque appel critique
- **Astuce** : Renouveler syst√©matiquement avant crawl/analyse

#### 4. **Endpoint `/urls` Bugu√©** ‚ö†Ô∏è
- **Probl√®me** : Impossible d'ajouter URLs apr√®s cr√©ation land
- **Solution** : URLs directement dans `start_urls` lors de cr√©ation
- **√âviter** : `POST /api/v2/lands/{id}/urls`

#### 5. **Bugs Asyncio dans Media Analysis & Readable** ‚ö†Ô∏è **NON R√âSOLU**
- **Probl√®me** : `RuntimeError: Task got Future attached to a different loop`
- **Fichiers affect√©s** :
  - `/app/app/tasks/media_analysis_task.py:51,237`
  - `/app/app/tasks/readable_working_task.py`
- **Erreurs associ√©es** : `InterfaceError: another operation is in progress`
- **Impact** : L'analyse m√©dia async et le pipeline readable sont instables
- **Workaround** : Utiliser uniquement le crawl sync sans ces fonctionnalit√©s
- **Status** : √Ä corriger - probl√®me de gestion des event loops asyncio dans Celery

#### 6. **Mots-cl√©s Obligatoires** ‚ö†Ô∏è
- **Probl√®me** : Sans mots-cl√©s, `relevance=0` pour toutes expressions
- **Solution** : Toujours ajouter termes via `/terms` apr√®s cr√©ation land
- **Impact** : D√©termine filtrage pertinence dans analyse m√©dia

#### 7. **DEPTH = Niveau de Crawl** üî• **CRITIQUE**
- **`depth: 0`** = Analyser m√©dias des **start_urls** seulement
- **`depth: 1`** = Analyser m√©dias des **liens directs** depuis start_urls
- **`depth: 2`** = Analyser m√©dias des **liens de liens** (2e niveau)
- **`depth: 999`** = Analyser **TOUS** les m√©dias sans limite de profondeur
- **‚ö†Ô∏è BUG ENDPOINT** : L'endpoint `/media-analysis-async` ignore le param√®tre `depth` et force toujours `depth: 999`

### üéØ **Workflow Anti-Erreurs (CRAWL SYNC)**

```bash
1. ‚úÖ Cr√©er land avec start_urls int√©gr√©es (pas d'endpoint /urls)
2. ‚úÖ Ajouter termes OBLIGATOIREMENT via /terms
3. ‚úÖ Renouveler token avant chaque action
4. ‚úÖ Lancer crawl avec POST /crawl (limit, depth optionnels)
5. ‚úÖ Attendre suffisamment (60s pour 5 URLs)
6. ‚úÖ V√©rifier logs Celery: docker logs mywebclient-celery_worker-1 --tail=50
7. ‚úÖ Utiliser le script de test: ./MyWebIntelligenceAPI/tests/test-crawl-simple.sh
```

**R√©sultats attendus (5 URLs Lecornu):**
- Duration: ~50 secondes
- URLs Processed: 5
- Errors: 0
- Content extrait: ~50,000 caract√®res
- Liens d√©couverts: ~1,200 liens
- M√©dias extraits: ~200-250 m√©dias

---

## üß™ Tests Manuels D√©taill√©s

### Test 1 : Authentification
```bash
TOKEN=$(curl -s -X POST "http://localhost:8000/api/v1/auth/login" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=admin@example.com&password=changeme" | jq -r .access_token)
echo "Token: $TOKEN"
```

### Test 2 : Cr√©ation Land Compl√®te
```bash
# Land avec URLs int√©gr√©es (√©vite l'endpoint /urls bugu√©)
LAND_ID=$(curl -s -X POST "http://localhost:8000/api/v2/lands/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "test_fonctionnel", 
    "description": "Test crawl fonctionnel",
    "start_urls": ["https://httpbin.org/html", "https://example.com"],
    "words": ["test", "example"]
  }' | jq -r '.id')
echo "Land cr√©√©: $LAND_ID"
```

### Test 3 : Ajout Mots-cl√©s (Obligatoire pour pertinence)
```bash
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/terms" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"terms": ["html", "test", "example", "title"]}'
```

### Test 4 : Crawl Simple
```bash
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/crawl" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"limit": 3}' --max-time 120
```

### Test 5 : Analyse M√©dia (ASYNC)
```bash
# Analyse rapide (expressions tr√®s pertinentes seulement)
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"depth": 0, "minrel": 3.0}'

# Analyse compl√®te (toutes expressions)  
curl -X POST "http://localhost:8000/api/v2/lands/${LAND_ID}/media-analysis-async" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"minrel": 0.0}'
```

## üß≠ Sc√©nario d'Usage Complet

### Script de Test Automatis√©
Le script `scripts/land_scenario.py` reproduit l'ancien workflow CLI :

```bash
python scripts/land_scenario.py \
  --land-name "MyResearchTopic" \
  --terms "keyword1,keyword2" \
  --urls "https://example.org,https://example.com" \
  --crawl-limit 25
```

**Variables d'environnement:**
- `MYWI_BASE_URL` (d√©faut: `http://localhost:8000`)  
- `MYWI_USERNAME` / `MYWI_PASSWORD` (d√©faut: `admin@example.com` / `changeme`)

### Migration depuis SQLite
```bash
# Migration d'une base SQLite existante
python scripts/migrate_sqlite_to_postgres.py --source /path/to/mwi.db
```
## üìö Documentation de r√©f√©rence

- [INDEX_DOCUMENTATION.md](INDEX_DOCUMENTATION.md) ‚Äî carte et statuts des documents actifs
- [QUALITY_SCORE_GUIDE.md](.claude/docs/QUALITY_SCORE_GUIDE.md) ‚Äî guide complet Quality Score System ‚ú® NOUVEAU
- [R√âSUM√â_CORRECTIONS_17OCT2025.md](R√âSUM√â_CORRECTIONS_17OCT2025.md) ‚Äî synth√®se produit & plan d'actions
- [TRANSFERT_API_CRAWL.md](TRANSFERT_API_CRAWL.md) ‚Äî audit complet et cartographie Legacy ‚Üí API
- [CORRECTIONS_PARIT√â_LEGACY.md](CORRECTIONS_PARIT√â_LEGACY.md) ‚Äî corrections techniques (m√©tadonn√©es, HTML, stockage)
- [Transfert_readable.md](Transfert_readable.md) ‚Äî suivi de la parit√© du pipeline readable
- [CHA√éNE_FALLBACKS.md](CHA√éNE_FALLBACKS.md) ‚Äî sch√©ma d√©taill√© des fallbacks d'extraction
- [METADATA_FIXES.md](METADATA_FIXES.md) ‚Äî corrections m√©tadonn√©es (journal complet)
- [CORRECTIONS_FINALES.md](CORRECTIONS_FINALES.md) ‚Äî synth√®se + plan de tests m√©tadonn√©es
- [compare_addterms_analysis.md](compare_addterms_analysis.md) ‚Äî √©tat des lieux AddTerms et recommandations
- [Architecture.md](Architecture.md) ‚Äî structure du d√©p√¥t et responsabilit√©s par module
- [GEMINI.md](GEMINI.md) ‚Äî guide op√©rateur/API (vue compl√©mentaire)

**Derni√®re mise √† jour**: 18 octobre 2025
**Version**: 1.2 (ajout Quality Score System)
**Mainteneur**: √âquipe MyWebIntelligence
